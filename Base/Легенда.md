Здравствуйте, меня зовут Линник Ярослав Максимович. Я Python Backend Developer с 5 годами комерческого опыта. Последнее место работы крупная retail компания Вкусвилл. 

В ней я занимался непосрественно разработкой движка мультиканальной синхронизации товарных остатков. Входил в состав кросфункциональной комманды, состоящй из 8 человек. 

В нее входили Teamlead, 2 backend, 2 frontend, devops, Qa тестировщик и inginer

Система в режиме реального времени собирала данные с ERPишек, скаладов и дарксторов и синхронизировала их между магазинами, мобильными сервисами и маркетплейсами.

В качестве архитектуры была выбрана событийная Event driven с CQRS использовали kafka, redis, PostgreSQL, Elastick Search, prometheus, Grafana,  Я занимался core логикой резервов и списаний, проекции для быстрого чтения и адаптеры к маркетплейсам с retiers и rare- limitarse.

Сложности были разные: гонки при массовых заказах, рассинхроны с ERP, пики нагрузки в акции. Мы решали их транзакциями и TTL-резервами, reconciliation job’ами и масштабированием consumer’ов Kafka. В итоге отмены заказов из-за отсутствия товара удалось снизить почти в десять раз, а время обновления остатков сократилось до менее секунды.

Делал документацию и онбординг гайд и занимался менторством для новичков.











Задачи:
- Проектировал и реализовывал систему микросервисов собирающих данные с ERP, WMS и дарксторов в единнную консистентную витрину.
- Настраивали Event driven + CQRS подход: команды (Reserve/Confirm/Release) → события в Kafka → обновление проекций для быстрых запросов наличия.
- Реализовывал адаптеры к внешним системам ( службе доставки) включая rate-limiters, retries и contract-тесты
- Написал антифрод-сервис для поиска аномалий по возвратам и лояльности, вместе с Data Science командой внедряли near real-time инференс
- Поднял сервис мониторинга корпоративных приложений (FastAPI + Prometheus + Grafana) с бизнес-метриками на дашбордах.

Ошибки:
- На старте система сильно страдала от **race conditions** при массовых заказах → внедрили транзакции с row-level locks в PostgreSQL и TTL-резервы в Redis
- Случались рассинхроны между ERP и проекциями → сделали reconciliation job с auto-fix для мелких расхождений и алерты на drift_rate.
- Были сложности с нагрузкой в пиковые часы (акции, распродажи) → внедрили горизонтальное масштабирование Kafka consumer’ов и throttling на API Gateway.
- Первые версии адаптеров для маркетплейсов часто падали из-за «кривых» внешних API → спасали retry с backoff и fallback на ручной режим через ops-UI.

Что получили :
- Снизили отмены заказов из-за отсутствия товара почти в 10 раз.
	
- Достигли обновления остатков во всех каналах за <1 секунды в 95% случаев.
    
- Сократили ручные корректировки склада на десятки процентов благодаря reconciliation.
    
- Повысили прозрачность работы всей корпоративной системы — бизнес впервые увидел живые метрики по запасам и заказам в дашбордах. 

**Доп. ссылки:** [[Вопросы к Легенде]], [[Вкусвилл]]

**Tags**: #IT  #ОМ 