## Мотивация

- Что я ищу при поиске компании/Идеальное место работы ЧАСТО
	- Хочу проект с современным Python-стеком и технологиями и реальным влиянием на архитектуру — и команду, с которой приятно работать и учиться друг у друга.

	- Хочу продукт, где технология — реальное конкурентное преимущество: высоконагруженные микросервисы, событийная архитектура (Kafka/RabbitMQ), продуманная работа с данными и интеграции с ML/LLM. Домены, которые нравятся: финтех и e-commerce (там есть осязаемые бизнес-метрики), логистика/retail (много потоков данных и near-real-time), а также платформы с прогнозированием спроса, антифродом и управлением ассортиментом. Важно иметь влияние на архитектуру (дизайн сервисов, очередей, схемы данных, выбор технологий) и работать в команде, где ценится инженерная культура: код-ревью, тесты, CI/CD, прозрачные процессы.

- На что смотришь в первую очередь при выборе компании
	- В первую очередь смотрю на сильный технический бренд компании, приятное общение с интервьюером и командой, интересные проекты и задачи, крутые процессы. И польза для пользователей.
	- ЗП и соцпакет - вторично.

	- Сильный технический бренд: зрелые практики (ADR, ревью, постмортемы), понятные стандарты кода, измеримые цели (SLO/SLI).
	- Команда: открытость обсуждениям, готовность к экспериментам (PoC, канареечные релизы), взаимовыручка.
	- Проект: реальные вызовы — highload, консистентность данных, антифрод, потоковая обработка.
	- Польза для пользователей: когда видно, что решения меняют поведение продукта (меньше OOS, быстрее витрины, меньше фрода).
	- Компенсация/льготы — вторично; важнее влияние и рост как инженера.

- От чего в своей работе кайфуешь больше всего? РЕДКО
	- Больше всего в работе меня драйвит возможность решать сложные технические задачи, которые реально влияют на продукт. Особенно нравится, когда результат заметен пользователям: например, оптимизация системы, которая ускоряет работу или делает использование удобнее. Вдохновляет, когда мои решения помогают бизнесу и улучшают пользовательский опыт — ощущаешь, что ты строишь что-то значимое, а не просто пишешь код.

- Что первостепенно: интересы бизнеса или интересы пользователей? ИНОГДА

	- Идеальный ответ – баланс.
	- Если PM предлагает резко поднять подписки:

		* Я попрошу обоснование: есть ли аналитика и гипотеза, почему это поможет бизнесу и не ухудшит Retention и удовлетворенность.

		* Предложу провести A/B-тестирование или постепенное повышение с замером обратной связи.

		* Объясню PM-у риски ухудшения лояльности и долгосрочных метрик, если решение окажется негативным для пользователей.

		* Моя задача – четко обозначить риски и помочь принять решение, которое сохранит баланс между интересами бизнеса и пользователей.

- Мои увлечения           ИНОГДА

	- В свободное время я занимаюсь в спортзале — это помогает держать форму, дисциплинирует и заряжает энергией. Люблю готовить и интересуюсь правильным питанием, для меня это способ заботиться о здоровье. Также люблю прогулки на свежем воздухе, Кемпинг которые помогают перезагрузиться после работы. Ну и, конечно, фильмы и сериалы — это мой способ расслабиться и иногда почерпнуть новые идеи.Ужасы.

- Когда готов выйти на работу ЧАСТО

	- Не ранее ,чем через 2 недели.
	- Когда вы хотите, чтобы я начал?

- Отношения к командировкам РЕДКО
	- Отношусь нейтрально, если они не чаще раза в несколько месяцев и положительно, если они в другие страны либо в города России, где что-то можно посмотреть красивое.

- Какой вы видите свою будущую команду? ЧАСТО
	- Хочу команду:

		1. В которой мы вместе сможем развиваться
		2. С которой мы вместе будем делать ваш классный мега продукт
		3. С которой мы вместе выстроим самые эффективные процессы

- Каким вы видите своего будущего начальника/руководителя? ЧАСТО
	- Я хотел бы работать с руководителем, у которого есть амбиции развивать продукт и команду, и который вдохновляет своим примером. Важно, чтобы в работе были взаимное уважение, доверие и открытость. Мне ценно, когда руководитель готов делегировать сложные задачи, поддерживать и давать честную обратную связь — это помогает расти и приносить больше пользы команде

## О Себе

- Почему я стал программистом
	- В програмировании меня зацепило, то что это творческая и развивающаяся сфера. В которой можно реализовываться как инженер и найти много знакомств.
	- Помогает от Альцгеймера, так какой был вопрос?)

- Расскажи о себе
	- Меня зовут Ярослав Линник, я **Python Backend Developer** с более чем **5-летним опытом коммерческой разработки**. последнее место работы компания Т-Банк, где занимался созданием и развитием микросервисов на **Python, FastAPI, PostgreSQL, Redis, RabbitMQ, Kafka и Kubernetes**.проектировал высоконагруженную платформу для доставки и управления ассортиментом, строил микросервисы и очереди, интегрировал ML/LLM-модули (онлайн-инференс) и проектировал API для прогнозирования спроса и антифрода.  
		**Стек:** Python, FastAPI, PostgreSQL, Kafka, SQLAlchemy, Redis, RabbitMQ, FastStream, Docker/Compose, Kubernetes, Pytest, CI/CD, asyncio.

	- В моей зоне ответственности — проектирование архитектуры сервисов, интеграция с другими системами, работа с потоками данных и оптимизация процессов. Среди ключевых проектов — система мультиканальной синхронизации товарных остатков в реальном времени, антифрод-сервис для выявления аномалий и сервис мониторинга корпоративных систем. Эти решения помогли бизнесу повысить прозрачность процессов и снизить операционные риски.

	- До этого я более двух лет работал в компании «Серконс» как backend-разработчик на **Django и DRF**. Там занимался созданием API для расчета физических формул и проведения испытаний автомобилей, интеграциями с внешними сервисами, а также разработкой DSL-модуля для инженеров. Получил опыт в области ML-инференса и генерации юридически значимых документов.

	- Учусь по направлению **Информационная безопасность** (бакалавриат).

	- Мне близки задачи, связанные с **разработкой надежных и масштабируемых backend-сервисов, интеграциями и оптимизацией бизнес-процессов**, а также взаимодействие с командами Data Science и DevOps.

-  2–3 ключевых достижения (контекст → решение → результат)

	Движок мультиканальной синхронизации товарных остатков (Т-Банк)

	Контекст: данные об остатках расходились между ERP/WMS/darkstore, витрина «плавала».

	Решение: асинхронный FastAPI-сервис + Kafka (события) + Redis (горячий кэш) + грамотная модель обновлений и партиционирования; продуманная схема консистентности.

	Результат: выдерживает ~2100 RPS, обновление остатков в реальном времени, единая консистентная витрина (сняло болевую точку мерчандайзинга и логистики).

	Антифрод-сервис (Т-Банк)

	Контекст: злоупотребления в возвратах/лояльности, ручные проверки, задержки.

	Решение: микросервис с near-real-time инференсом CatBoost/LightGBM, события через Kafka, пайплайн с MLflow, API для решений.

	Результат: автоматизация проверок, уменьшение злоупотреблений; ускорение реакции на аномалии.

	DSL-движок инженерных расчётов (ПроммашТест)

	Контекст: инженерам требовались новые сценарии испытаний без участия разработчиков.

	Решение: язык описания формул/таблиц по ГОСТ/ISO, интерпретатор на Python, автоподстановка показаний с DAQ-систем, проверки прочности/износа/усталости.
	
	Результат: инженеры стали самостоятельно строить сценарии; ускорение подготовки испытаний, меньше узких мест на стороне разработки.

- Твои сильные и слабые стороны в тех‑скиллах?       ИНОГДА   

	- Сильные стороны:

		- Опыт построения асинхронных систем и очередей (Celery, Kafka, RabbitMQ) с ретраями и fallback-механизмами.  
      
		- Уверенное владение экосистемой Python (Django, DRF, FastAPI, Celery, pytest).  
      
		- Проектирование и оптимизация микросервисной архитектуры (декомпозиция монолита, интеграция через REST и Kafka).  
      
		- Практический опыт работы с базами данных (PostgreSQL, ClickHouse) и построения ETL-процессов.  
      
		- Опыт настройки мониторинга и алертинга (Prometheus, Grafana, Sentry).  
      
		- Работа в контейнеризированной среде (Docker, Kubernetes), CI/CD и деплой.  
       
	- Слабые стороны:

		- Меньше опыта в низкоуровневом программировании и оптимизации работы ОС.  
      
		- Ограниченный опыт фронтенда — на уровне базового понимания для взаимодействия с backend.

- Что в вашей работе для вас самое сложное?
	- Проектирование и продумывание архитектуры, Нужно многое учесть и знать Масштабируемость, Надежность, Баланс между скоростью разработки и удобством системы.

- Три положительных и три отрицательных качества
	- Положительные
		- Ответственность. Ответственно подхожу кзадачам всегда стараюсь в срок и как можно лучше.
		- Усидчивость. Готов разбираться в чем то новом или работать над сложной задачей пока не будет результата.
		- Коммуникабельность. Всегда настроен на общение и обсуждение. Люблю новые знакомства и делиться опытом.
	- Отрицательные
		- Перфекционизм. Довожу все до излишнего идеала или закапываюсь в технологию глубоко на что уходит много времени.
		- Плохо переншу рутину. Ежедневные однотипные задачи могут погрузить в апатию.
		- Неуверенность. Долго думаю какой вариант написания лучше из за этого могу стагнировать и стоять на месте долго думая над хорошим решением.

## Команда, процессы, Задачи

- Каков состав команды?
	- т-банк - 6 человек. TeamLead, 2 Backend, 1 Frontend, Devops, QA.
	- Sercons - 3 backend, Аналитик и фронтенд. Подключались Devops и QA со стороны.

-  Моя Методология            РЕДКО
	- Мы работали по Scrum с двухнедельными спринтами. Обязательные события: планирование спринта (определяли задачи на 2 недели), груминг (уточняли требования и дробили задачи на подзадачи), дейлики (каждый рассказывал, что сделал и какие есть блокеры), обзор спринта в конце (показывали проделанную работу команде и заказчику) и ретроспектива в конце (обсуждали, что улучшить в процессе). Бэклог вёл продакт-оунер, все задачи брали только из него. Если возникали баги вне плана, они попадали в ближайший спринт через перепланирование.
	- Процесс был более гибкий, смесь Scrum и Kanban. Основные этапы — двухнедельное планирование, ежедневные созвоны по прогрессу и демо для заказчика(в конце спринта). Приоритетные задачи от заказчика могли попадать в работу быстрее, чем в чистом Scrum, поэтому подход адаптировали под ситуацию

- Откуда приходят задачи:
	-   Задачи к нам приходили через Product Owner, но формулировал и прорабатывал их аналитик. Он собирал требования от бизнеса, оформлял их в Jira, а уже на грумингах мы вместе с аналитиком уточняли детали и дробили крупные фичи на подзадачи: бэкенд-эндпоинты, миграции БД, интеграции с Kafka и так далее
	- в обеих компаниях практиковалось кросс-ревью. Каждый merge request обязательно проверяли минимум два разработчика, и только после двух аппрувов код сливался в main.

- Как оцениваете сроки выполнения задач
	- Обычно я начинаю с того, что уточняю цель задачи и выделяю непонятные моменты. Если задача большая, мы обсуждаем её на груминге и делим на подзадачи: эндпоинты, миграции БД, интеграции с очередями и т.п. После этого я прикидываю трудозатраты, накладываю риски (например, интеграция с внешним сервисом может затянуться), и на основе этого даю оценку. При необходимости добавляю небольшой буфер времени, чтобы не сорвать сроки из-за неожиданных проблем.

- Что такое Jira
	- Jira — это популярная система для управления задачами и разработкой, созданная компанией Atlassian. Она помогает организовывать рабочие процессы, вести бэклог, назначать задачи команде и отслеживать сроки. Jira поддерживает гибкие методологии, такие как Scrum и Kanban.
	- Мы в проектах использовали её для планирования спринтов, грумингов и ведения задач: Product Owner заводил фичи, а мы на основе них дробили работу на подзадачи и брали их в спринт.

- Что такое Confluence    РЕДКО

	- Confluence — это веб-приложение для совместной работы от Atlassian. Его используют для создания, обмена и управления документацией, а также для хранения знаний внутри команды.

	- Мы в проектах вели в Confluence техническую документацию: архитектурные схемы, описание API, инструкции по деплою и ретроспективы. Это было удобно, потому что у всех разработчиков и смежных ролей (QA, DevOps) была единая точка входа в знания.

- Что такое Scrum?
	- Scrum — это фреймворк гибкой разработки, который строится на итерациях фиксированной длины — спринтах (обычно 1–2 недели). В Scrum есть роли: Product Owner, Scrum Master и команда разработки. Основные артефакты — бэклог продукта и бэклог спринта.  
	- Церемонии включают планирование спринта, груминг (уточнение требований и разбиение фич на подзадачи), ежедневные короткие созвоны, обзор спринта и ретроспективу. В конце каждой итерации команда должна предоставить рабочий инкремент продукта, который можно показать заказчику.

	- В моём опыте в Т-банке мы как раз работали по Scrum: у нас были двухнедельные спринты, планирование, груминги, ежедневные стендапы, в конце спринта обзор и ретроспектива. Это помогало держать команду синхронизированной и регулярно улучшать процесс.

- Что такое Kanban?
	- Kanban — это метод управления задачами в потоке. Задачи визуализируются на доске («To Do», «In Progress», «Done»), и команда берёт новые задачи по мере готовности. Канбан помогает избегать перегрузки команды за счёт ограничения количества задач в работе (WIP-лимиты) и позволяет быстро реагировать на изменения приоритетов. В отличие от Scrum, у Kanban нет жёсткой привязки к спринтам, процесс идёт непрерывно.

	- В моём опыте в Aston (MedStream) мы использовали смесь Scrum и Kanban. У нас были двухнедельные спринты, но при этом приоритетные задачи от заказчика могли попадать в работу сразу, без ожидания следующего планирования. Такой гибрид помогал нам держать стабильный процесс и в то же время оперативно реагировать на запросы клиента.

- Руководство в обход менеджера ставит срочные задачи РЕДКО

	- Это бывает, и я отношусь к этому спокойно — главное правильно расставить приоритеты и держать прозрачность. В таких случаях я действую так:

	1. Сначала уточняю критичность задачи. Если это срочная блокирующая проблема (например, упала очередь или сломался сервис), я сразу предупреждаю команду и менеджера и переключаюсь на решение.  
      
    
	2. Если задача важная, но не критичная, я обсуждаю её с Product Owner и командой, чтобы понять, как корректно встроить её в текущий план.  
      
    
	3. Если нет возможности обсудить, я беру максимум информации, меняю приоритеты и обязательно уведомляю тех, кто ждёт мою работу, о сдвигах сроков.  
      
    

	- В любом случае я напоминаю, что корректно ставить задачи через Product Owner или тимлида, чтобы команда оставалась синхронизированной.

- Каждый день меняются задачи и приоритеты       СРЕДНЕ
	- Я сталкивался с такими ситуациями, особенно в аутсорс-проектах. В первую очередь я обсуждаю с менеджером и Product Owner, какие именно задачи стали приоритетными, и подсвечиваю риски по текущим задачам: что успею, а что придётся отложить. Если действительно есть критичная «горящая» задача, я переключаюсь на неё, но обязательно уведомляю команду и тех, кто ждёт результаты по другим задачам, что сроки смещаются и почему.

	- Такой подход помогает сохранять прозрачность, и команда понимает, что приоритеты меняются осознанно, а не хаотично.

- Расскажи о последнем ярком изменении в проекте
	- Полный ответ:  
	- Самым заметным изменением за последнее время для меня стало вынесение модуля профилей пользователей из монолита в отдельный сервис. Сначала у нас была проблема: изменения в профилях часто ломали другие части системы, плюс было сложно масштабировать нагрузку и обеспечить изоляцию персональных данных.  
	- Я спроектировал новый сервис на FastAPI, мы продумали миграции данных и отдельный слой авторизации, настроили взаимодействие через REST и Kafka.  Первые недели были непростыми: приходилось отлаживать схему взаимодействия с другими сервисами и решать конфликты при миграции. Но в итоге результат оказался очень полезным: персональные данные стали изолированы, работать с моделью профиля стало проще, а нагрузка на монолит снизилась.  
	- В целом этот переход помог команде быстрее развивать функционал, а система стала более масштабируемой и отказоустойчивой.

	- Краткий вариант:  
	- Недавно мы вынесли модуль профилей пользователей из монолита в отдельный сервис. Я спроектировал архитектуру, сделал миграции и взаимодействие через REST и Kafka. Благодаря этому данные стали изолированы, а команда получила более гибкую и масштабируемую систему.

- Как обычно делились обновлениями, статусами? РЕДКО

	- В нашей компании процесс обмена информацией в целом организован через несколько основных каналов, однако есть моменты, которые можно улучшить.

	Основные каналы коммуникации — мы активно используем Telegram и Confluence для общения и хранения документации. В Telegram создаются каналы для разных проектов и команд, что позволяет быстро получать информацию о текущем статусе задач, делиться обновлениями и обсуждать проблемы. Confluence используется для более глубокой документации, описания процессов и технических решений. Этот подход позволяет нам централизованно хранить информацию, но все-таки иногда возникает проблема, когда важные сообщения теряются среди общей массы.

	Регулярные встречи — у нас есть еженедельные митинги команд, где обсуждаются текущие задачи, проблемы и достижения. Это хорошая практика, но порой информация на таких встречах может быть немного перегружена или не всегда актуальна для всех участников. Иногда важно больше внимания уделять не только статусу задач, но и обмену знаниями между командами, особенно по техническим вопросам.

	Проблемы обмена информацией — одна из проблем, с которой сталкиваемся, — это когда информация не всегда доходит до тех, кто в ней действительно нуждается. Например, бывает, что решения, принятые на высшем уровне или в одной команде, не сразу становятся известными остальным, что может привести к дублированию усилий или недопониманию. Это особенно актуально, когда команды работают над похожими задачами, но не всегда синхронизируют свои действия.

	Что бы я изменил? В будущем я бы хотел улучшить процессы обмена информацией через более четкие и структурированные механизмы. Например, внедрить ежедневные или более частые синхронизации по проектам между командами, чтобы не только отслеживать статус задач, но и делиться ключевыми инсайтами, новыми подходами или решениями. Также хотел бы усилить роль встреч, на которых мы могли бы обсуждать не только текущие задачи, но и общий взгляд на архитектуру и развитие системы.

	В идеале мне бы хотелось, чтобы все ключевые решения и изменения в проекте сразу же документировались и стали доступны для всей команды, чтобы каждый был в курсе текущих направлений и мог своевременно внести свои предложения.

- Как взаимодействовал с другими командами? РЕДКО

	Я всегда придерживаюсь принципов прозрачности и чётких границ ответственности. Если за какой-то модуль отвечала другая команда, мы согласовывали интерфейсы через API и документацию, чтобы избежать двусмысленности.

	В Twinby у нас были отдельные команды, например, ML-инженеры, которые делали сервис модерации. Мы общались через заранее описанные контракты (REST + Kafka) и регулярно синхронизировались на статус-митингах и в мессенджерах. Если их сервис падал, я помогал отлаживать интеграцию или настраивал fallback, чтобы общий процесс не вставал.

	В Aston (MedStream) тоже приходилось работать с QA и DevOps из общего пула. Мы заранее договаривались о процессах деплоя и тестирования, чтобы не терять время на «передавании задач».

- Расскажи пример когда решал задачи с высокой степенью неопределенности?

	Одна из задач с высокой степенью неопределённости была в Т банке, когда мы внедряли модерацию контента через ML-сервис. На старте было непонятно:  
	– какой процент запросов будет обрабатываться вручную,  
	– насколько стабильно работает ML,  
	– и какие сценарии ошибок нужно предусмотреть.

	Мы начали с прототипа очереди на Celery + RabbitMQ и проверили поток данных на тестовой группе. Потом добавили ретраи и fallback: если ML не отвечает, контент проверяется по упрощённым правилам. Это дало стабильность даже при сбоях сервиса.

	В итоге система выдержала рост нагрузки, количество ручных проверок снизилось, а бизнес получил экономию времени модераторов. Для себя я вынес, что при неопределённости лучше идти через MVP, быстрые проверки и поэтапное снятие рисков.

- Ситуация когда не укладывались в сроки
	Такие ситуации случаются, особенно если задача неопределённая или в процессе всплывают неожиданные сложности. Для меня главное — держать прозрачность. Я сразу предупреждаю тимлида и команду, оцениваю новый срок и предлагаю варианты: перенести часть задачи в следующий релиз, привлечь помощь или работать сверхурочно, если это действительно критично.

	Например, в Twinby при вынесении профилей в отдельный сервис мы недооценили объём миграций БД. Когда стало ясно, что в срок не укладываемся, я сообщил команде и предложил разделить миграцию на этапы: сначала вынесли базовую часть, а менее критичные поля перенесли в следующий релиз. В итоге сервис вышел вовремя, а доработки сделали позже без давления.

- Действия в конфликтах

	Бывают спорные ситуации, но я стараюсь относиться спокойно и с эмпатией. Обычно причина в том, что коллеги смотрят с разных сторон или не хватает контекста. Был случай на код-ревью, когда коллега довольно эмоционально воспринял мои замечания. Я не стал спорить, а сел с ним и вместе разобрал задачу. Оказалось, что он не до конца понял требования, а я мог бы объяснить их чётче. Когда всё прояснили, ситуация быстро решилась, и дальше мы отлично работали вместе.  
  
	У нас была ситуация с продукт-менеджером: он хотел реализовать отчёт простым синхронным запросом, но я понимал, что при большом объёме данных API просто «ляжет». Мы разошлись во мнениях, поэтому я предложил быстрый эксперимент: сделал два прототипа и показал на цифрах, что потоковая выдача и фоновая генерация безопаснее. Когда он увидел результат на демо, сам согласился с моим вариантом.

- Бывало ли такое, чтоб у тебя была затруднена коммуникация с кем-то на работе? Как решал?

	Да, такие ситуации бывали. Один раз выполнение моей задачи блокировалось фичей другой команды, а сроки уже сильно поджимали. Я несколько раз мягко напоминал коллегам об этой блокировке и уточнял сроки, но долго не получал обратной связи. Тогда я обратился к руководителю, чтобы он помог расставить приоритеты между командами. После этого коммуникация наладилась, и задача была выполнена в срок.

- Получал ли ты когда-нибудь отрицательный фидбек? Если да, то какой?

	Да, у меня был отрицательный фидбек. Однажды я слишком долго разбирался с оптимизацией SQL-запроса, хотя задача была срочная — нужно было в первую очередь закрыть баг. Мне сказали, что важно сначала решить критичную проблему, а уже потом углубляться в оптимизацию. После этого я стал чётче расставлять приоритеты и сначала отдаю рабочее решение, а уже потом занимаюсь улучшениями.

	Да, такое бывало. Например, однажды я столкнулся с блокером в чужом коде и пытался решить его сам, из-за чего задержал выполнение своей задачи. На ретроспективе мне сказали, что лучше было сразу вынести проблему на обсуждение. С тех пор я стараюсь сразу сигнализировать о блокерах, чтобы команда могла помочь и процесс не вставал.

- Один или в команде

	Да, мне доводилось работать и самостоятельно, и в команде. Например, во фриланс-проектах и небольших задачах я был один, и это помогло прокачать самодисциплину и умение быстро находить решения. Но в целом я считаю, что командная работа продуктивнее: есть обмен опытом, возможность обсудить архитектурные решения, а результат получается качественнее.

	В Twinby мы как раз работали кросс-функциональной командой, и мне нравилось помогать коллегам и учиться у них. Для меня важно, что в команде мы вместе двигаемся к общей цели, а не просто делаем набор отдельных задач.

- Неадекватные требования заказчика к проекту

	Бывают ситуации, когда требования заказчика кажутся неадекватными. Обычно это связано либо со сроками, либо с самим функционалом.

	Если дело в сроках — я стараюсь обсудить задачу и предложить варианты: либо увеличить сроки, либо привлечь дополнительные ресурсы. Если такой возможности нет, то беру на себя ответственность и, при необходимости, готов работать сверхурочно, чтобы уложиться.

	Если же речь про функционал, который неудобен для пользователя, я стараюсь смотреть на ситуацию с его стороны. В таких случаях формирую встречные предложения и обсуждаю их с заказчиком. Часто после конкретных аргументов и примеров удаётся найти решение, которое и заказчику подходит, и пользователям удобно.

- Бывали ли такие ситуации, чтоб ты был не согласен с кем-то? Как действовал в такой ситуации?

	В Twinby была ситуация при разработке сервиса фоновой аналитики: продукт-менеджер хотел реализовать выгрузку отчётов через синхронный запрос, чтобы «побыстрее показать бизнесу». Я понимал, что такой подход небезопасен — отчёты могли быть большими, API просто зависал бы или давал 502-ошибки. Сначала я выслушал его аргументы, потом предложил компромисс: сделать быстрый прототип двух вариантов — синхронного и асинхронного (с генерацией отчёта через Celery и сохранением в PostgreSQL/ClickHouse). Мы прогнали тесты, посмотрели время ответа и нагрузку. Цифры показали, что асинхронная модель гораздо стабильнее и не требует жертвовать сроками. После демо продукт сам согласился на этот вариант.

- Доводилось ли тебе работать с неэффективным процессом? Если да, то что делал в такой ситуации?

	Да, с неэффективными процессами сталкивался. Например, в Aston у нас CI-пайплайн собирался почти полчаса. Я проанализировал шаги, сократил лишние проверки и добавил кэширование зависимостей — время сборки сократилось примерно вдвое.
	
	В Twinby была история с дейликами: они часто затягивались на час и превращались в обсуждение «всего подряд». Я предложил жёсткий таймбокс на 15 минут и фиксированную повестку («что сделал, что планирую, есть ли блокеры»). В итоге встреча стала короче и информативнее, а детали мы обсуждали уже точечно после.

	Были и технические улучшения: например, раньше в разных сервисах стояли разные линтеры. Я предложил единый конфиг и внедрил ruff для всех проектов, что упростило ревью и сделало кодстайл единым.

	В целом процессы в командах были сильные, я просто помогал убрать лишние тормоза и сделать их чуть более эффективными.

- В каком проекте ты бы точно не хотел работать?

	Не могу сказать, что есть проекты, где я совсем не хотел бы работать. В команде всегда бывают разные задачи, и если это нужно для результата, я готов помочь. Конечно, мне ближе именно бэкенд-разработка, и я развиваюсь в этом направлении. Задачи по фронтенду или DevOps мне не так интересны, но при необходимости могу в них разобраться и выполнить.

- Задачи, которые тебя вдохновляют?       СРЕДНЕ

	Меня вдохновляет любая задача, которая приносит пользу бизнесу и пользователям. Но больше всего меня заряжают:

	Технически сложные задачи с проектированием архитектуры. Например, в Twinby при вынесении профилей в отдельный сервис я участвовал в проектировании взаимодействия через REST и Kafka.  
      
	Оптимизация производительности и надёжности. Было приятно видеть, как в Aston CI-процессы и сервис хранения файлов стали стабильнее после доработок с chunk-загрузкой и retry-механикой.  
    
	Задачи с прямым эффектом для пользователей. Например, внедрение сервиса модерации в Twinby снизило ручные проверки и ускорило реакцию системы, что сразу заметили пользователи.

- Что бы ты сделал для того, чтобы улучшить процессы в команде? Что делал?

	В Twinby мы заметили, что нужная информация была разбросана по чатам и гугл-докам. Я создал структуру в Confluence: архитектура, процессы, ADR-файлы («почему Kafka», «почему ClickHouse») и страницу «Первые шаги». Теперь новому разработчику не нужно выспрашивать детали — он сам поднимает проект за день.

	Также мы внедрили единый стиль кода: ruff + mypy, подключенные через pre-commit и CI. Это убрало мелкие споры на ревью и сфокусировало обсуждения на архитектуре.

	В Aston мы упорядочили ветвление: оставили main, feature/* и hotfix/*. Добавили MR-шаблон с чек-листом (линтер✓, тесты✓, дока✓). Релизы стали чаще и без «мерджа ради мерджа».

	Параллельно я помог QA настроить автопроверку Swagger-контрактов — баги ловились ещё на CI, а не в проде.

	Итог: процессы стали прозрачнее, ревью быстрее, а команда работала стабильнее.

## Технический стек и инфраструктура.

- Стек и архитектура

	- Т-банк

		**Стек:** Python, **FastAPI**, **PostgreSQL**, **SQLAlchemy**, **Kafka**, **RabbitMQ**, **Redis**, **FastStream**, **Asyncio**, **Pytest**, **Docker** / docker-compose, **Kubernetes (k8s)**, **CI/CD** (pipeline с линтами и тестами), (интеграции с ML/LLM: **CatBoost/LightGBM**, **MLflow** для трекинга).

		**Архитектура:** микросервисная платформа для e-commerce/логистики и контуров лояльности. Сервисы построены на **FastAPI**, общение синхронно по **REST** и асинхронно по **Kafka** (событийная шина). Для фоновых/долгих задач — **RabbitMQ** + воркеры (через **FastStream**/Celery-подобный паттерн).  
		Хранилище — **PostgreSQL** (аккуратные индексы и партиционирование под горячие таблицы), оперативное состояние и кэши — **Redis**. Параллельно — интеграции с ML-модулями для **near-real-time** инференса (антифрод, прогнозы спроса), версионирование/метрики моделей через **MLflow**. Развёртывание в **Kubernetes** с горизонтальным масштабированием сервисов и воркеров.

		---

		## Примеры сервисов

		### 1) **Inventory-Sync Service**

		Микросервис мультиканальной синхронизации остатков (ERP / WMS / darkstore).

		- **Назначение:** агрегировать изменения из разных источников, нормализовать и публиковать в единую консистентную витрину остатков.
    
		- **Технологии:** FastAPI, Kafka (топики `inventory.*`), Redis (горячий кэш), PostgreSQL (истина состояния/вторичные срезы), Asyncio/FastStream для консьюмеров.
    
		- **Ключевые решения:**
    
		    - разделение топиков по критичности (горячие «оперативные» vs «bulk» обновления),
        
		    - партиционирование в БД по складу/региону или SKU-группе,
        
		    - идемпотентность апдейтов (события с версионированием/лампорты/offsetы, дедупликация).
        
		- **Нагрузка:** выдерживает **~2100 RPS** на пиках (из резюме), обновления — **в реальном времени**.
    

		### 2) **Anti-Fraud Service**

		Сервис выявления аномалий по возвратам и злоупотреблениям лояльностью.

		- **Назначение:** онлайн-оценка событий транзакций/возвратов и решений по кейсам с минимальным участием ручной модерации.
    
		- **Технологии:** FastAPI (внешний REST API/внутренние endpoints), Kafka (входящие события), Redis (краткоживущий кэш/скореры), MLflow (артефакты и метрики), CatBoost/LightGBM (инференс).
    
		- **Ключевые решения:**
    
		    - **near-real-time** инференс с бюджетом по latency,
        
		    - fallback-механики (правила/скоринговые эвристики), если ML недоступен,
        
		    - событийный аудит (решение, факторы, версия модели) → PostgreSQL.
        

		### 3) **Demand-Forecasting / Stock-Planning Services**

		Контур прогнозирования спроса и формирования запасов.

		- **Назначение:** потоковая подготовка признаков по продажам/поставкам, пакетное обучение, публикация прогнозов в витрины запасов.
    
		- **Технологии:** FastAPI (ингест/фичи), Kafka (ETL-события), PostgreSQL (фичсторы/витрины), MLflow (эксперименты/версии моделей).
    
		- **Ключевые решения:** разнос онлайн-путей (запросы пользователей) и оффлайн-расчётов в очереди/батчи; аккуратная консистентность при замене версии модели.
    

		### 4) **Events-Ingestion / Orchestrator** (внутренний)

		- **Назначение:** стандартизированный приём внешних событий, нормализация payload, маршрутизация в нужные топики/воркеры.
    
		- **Технологии:** FastAPI, Kafka, FastStream, Redis.
    
		- **Ключевые решения:** единая схема событий, трейс-контекст, ретраи/Dead-letter для «ядовитых» сообщений.
    

		---

		## Пример бизнес-потоков

		### A) Обновление остатков и антифрод параллельно
		
		1. **ERP/WMS** фиксирует изменение → публикует событие в **Kafka** (`inventory.updated`).
    
		2. **Inventory-Sync** читает событие, нормализует, обновляет **PostgreSQL** и **Redis** (горячий кэш витрины), при необходимости порождает вторичные события (`inventory.projected`).
    
		3. **Anti-Fraud** подписан на релевантные типы событий (например, возвраты, подозрительные операции с остатками/заказами) и выполняет **near-real-time** инференс.
    
		4. При сбое (таймаут ML/брокер): фиксируется **метрика/лог**, срабатывает **fallback**, решение маркируется как «degraded», событие/ошибка уходит в DLQ — последующая отработка по регламенту.
    

		### B) Прогноз → план закупок/выкладки

		1. ETL-сервис формирует **фичи** по продажам/поставкам → пишет в БД и публикует событие `features.ready`.
    
		2. Сервис **обучения/инференса** запускает расчёт (батч) → регистрирует модель/метрики в **MLflow**, записывает прогнозы.
    
		3. **Stock-Planning** обновляет витрины (REST) для систем закупок/мерчандайзинга.
    
		4. При переключении версии модели — канареечный/постепенный rollout с мониторингом дельты метрик (ошибка прогноза, OOS-rate).
    

		---

		## Результаты (по резюме и доп. детализация)

		- **Реальное время обновления остатков** и единая консистентная витрина (снятие рассинхронов ERP/WMS/darkstore).
    
		- **Высокая нагрузка:** поток выдерживает **~2100 RPS** без деградации критичных путей.
    
		- **Антифрод:** near-real-time инференс сократил долю злоупотреблений по возвратам/лояльности, часть ручных проверок ушла.
    
		- **Прогнозирование запасов:** уменьшение **out-of-stock** и более точные закупки при сохранении стабильности основного контура.
    

		---

		## Практики качества и эксплуатации

		- **Тестирование:** Pytest (юнит), интеграционные сценарии (docker-compose: Postgres/Redis/Kafka/RabbitMQ), для критичных путей — нагрузочные профили (latency/черезput/consumer lag).
    
		- **CI/CD:** линтеры/типы → тесты → сборка Docker-образов → выкладка на dev/stage → smoke. Prod — через ручной gate (быстрый rollback при рисках).
		    
		- **Наблюдаемость:** структурированные логи (JSON, trace_id/request_id, сервис, путь, результат), централизованный сбор; технические метрики (error rate, latency, consumer **lag**, retries, DLQ), бизнес-метки (доля фрода, обновление витрины, OOS-rate).
    
		- **Устойчивость:** ретраи с backoff, **dead-letter** для несъедобных сообщений, TTL кэшей, идемпотентность при повторной доставке, фоллбэк-ветки на случай недоступности ML/внешних интеграций.
    
		- **Скейлинг:** **k8s HPA**, в том числе триггеры по **consumer lag** для консьюмеров Kafka/RabbitMQ; разнесение «тяжёлых» и «лёгких» потоков по топикам/очередям и консьюмер-группам.
    
		- **Данные:** БД — продуманное партиционирование, индексы под горячие запросы, аккуратные миграции; **Redis** — для горячих срезов и rate-limit/счётчиков.
    

		---

		## Безопасность и доступы

		- **RBAC/JWT**: матрица прав на уровне API, проверка ролей/политик, кэш решений в Redis, аудит-лог доступа и решений антифрода.
    
		- Секреты/ключи — через безопасное хранилище; внешние интеграции — по защищённым каналам.
    
		- Для моделей — управление версиями в **MLflow**, контроль совместимости входных/выходных контрактов.
    

		---

		## Что именно я делал (роль и вклад)

		- Проектирование **архитектуры** ключевых сервисов (Inventory-Sync, Anti-Fraud, Forecasting), выбор технологий, схемы очередей/топиков, стратегии консистентности.
    
		- Реализация **ядра микросервисов** на **FastAPI** с **Asyncio/FastStream**-консьюмерами, интеграция Kafka/RabbitMQ, **Redis**-кэшей, **PostgreSQL** (ORM: SQLAlchemy).
    
		- Встраивание **ML-инференса** (CatBoost/LightGBM) с **near-real-time** SLA и **fallback**-механиками; трекинг через **MLflow**.
    
		- Настройка **сквозных тестов** и правил для CI/CD, проработка сценариев отказа/отката, разруливание «залипаний» в брокерах (разделение потоков, DLQ, prefetch/ack).
    
		- Оптимизация производительности (холод/горячие пути, кэширование, разнос потоков, корректные индексы/партиции), что позволило держать **~2100 RPS** без потерь по стабильности.
    
	- Серконс

	**Стек:** Python, Django, DRF, PostgreSQL, Kafka, Redis, Celery, RabbitMQ, Docker, Nginx, unittest

	**Архитектура:** изначально был **монолит на Django**, но постепенно стали выносить отдельные функции в модули и сервисы. Для интеграций использовались **REST API** и очереди сообщений (**Kafka, RabbitMQ**). Celery применялся для тяжёлых фоновых задач (генерация отчётов, шифрование, интеграции с внешними системами).

	**Примеры сервисов и модулей:**

	- **Formula-Engine** — DSL-модуль для задания формул и таблиц допуска по ГОСТ/ISO, автоматически считал характеристики по показаниям с DAQ.
    
	- **ML-Inference Module** — встроенный Django-модуль для онлайн-инференса catboost модели (прогноз отказа испытаний).
    
	- **Report-Service** — генерация многоязычных протоколов испытаний, их шифрование и отправка на подпись в Госключ, с последующей проверкой статуса.
    

	**Пример бизнес-потока:**  
	Инженер задаёт формулу в DSL → система получает показания с датчиков (DAQ) → Formula-Engine рассчитывает характеристики → при этом параллельно ML-модуль прогнозирует вероятность отказа испытания → итоговый протокол формируется в Report-Service → шифруется и уходит на подпись в Госключ.

	**Результаты:**

	- автоматизировали расчёты и снизили нагрузку на инженеров,
    
	- внедрили предиктивную аналитику качества испытаний,
    
	- упростили документооборот с юридически значимой подписью.

- Какие были микросервисы за что отвечали и как взаимодействовали

	- Вкусвилл

		**1. Inventory-Sync Service**

		- **Задача:** синхронизация товарных остатков из ERP, WMS и darkstore-ов в единую витрину.
    
		- **Взаимодействие:** слушал события в **Kafka**, агрегировал данные и складывал в PostgreSQL/Redis. Другие сервисы брали данные через REST.
    

		**2. Anti-Fraud Service**

		- **Задача:** отслеживание аномалий (возвраты, злоупотребления в лояльности).
    
		- **Взаимодействие:** получал события о транзакциях из Kafka → гонял через ML-модель → публиковал результат обратно в Kafka.
    

		**3. Monitoring Service**

		- **Задача:** мониторинг работы корпоративных сервисов, сбор бизнес-метрик.
    
		- **Взаимодействие:** собирал данные по REST и метрикам сервисов, публиковал в **Prometheus**, визуализировал через **Grafana**.
    

		**4. Вспомогательные сервисы (Celery + RabbitMQ)**
	
		- **Задачи:** фоновые операции (например, пересчёты агрегатов, отложенные задачи, нотификации).
    
		- **Взаимодействие:** сервисы складывали задачи в очереди RabbitMQ, Celery-воркеры забирали их и возвращали результат по REST или Kafka.
    

		👉 Общая схема:  
		ERP/WMS → Kafka → Inventory-Sync → (REST/Redis/Postgres) → другие сервисы  
		Транзакции → Kafka → Anti-Fraud → Kafka → Profile/CRM  
		Все сервисы → Prometheus/Grafana через Monitoring

	-  Серконс (микросервисы и взаимодействие)

		**1. Formula-Engine (DSL-модуль)**

		- **Задача:** инженеры задавали формулы по ГОСТ/ISO, система считала характеристики по датчикам DAQ.
    
		- **Взаимодействие:** жил внутри монолита на Django, данные получал по REST от DAQ и отдавал результат другим модулям.
    

		**2. ML-Inference Module**

		- **Задача:** прогноз “провала” испытания по catboost-модели.
    
		- **Взаимодействие:** получал данные из Formula-Engine, запускал онлайн-инференс, результат сохранял в БД и отправлял уведомления через Kafka.
    

		**3. Report-Service**

		- **Задача:** формирование многоязычных протоколов испытаний, шифрование и подписание через Госключ.
    
		- **Взаимодействие:** получал данные по REST из Formula-Engine и ML-модуля, складывал задачи в **RabbitMQ**, Celery-воркеры выполняли генерацию/шифрование, результат сохранялся в PostgreSQL и отправлялся наружу.
    

		**4. Интеграционные сервисы**

		- **Задачи:** общение с Госключ, ChineseCFCA и другими внешними системами.
    
		- **Взаимодействие:** асинхронное через Celery + RabbitMQ, синхронное через REST API.
    

		👉 Общая схема:  
		DAQ → Formula-Engine → ML-Inference → Kafka (события прогноза)  
		Formula/ML → Report-Service → RabbitMQ → Госключ/CFCA

	
		📌 У тебя получается красивая логика:

		- В **ВкусВилле** микросервисы были ориентированы на **операционные процессы (товары, возвраты, мониторинг)** и активно строились на FastAPI + Kafka.
    
		- В **Серконс** больше упор был на **инженерные расчёты, документы и интеграции** внутри Django-монолита, но с элементами микросервисов через Celery + RabbitMQ + внешние API.

- Принцип выбора взаимодействия сервисов

	Я придерживаюсь простого правила:

	- **REST (синхронно)** — если нужно мгновенно ответить пользователю или другой системе.
    
	- **Kafka (асинхронно)** — если это событие, на которое можно отреагировать позже, либо нужно оповестить сразу несколько сервисов.
    
	- **RabbitMQ + Celery (асинхронные задачи)** — если это тяжёлая или долгоживущая операция, которую нужно гарантированно выполнить, но не в онлайне.
    

	REST

	Использую, когда:

	- требуется быстрый ответ (например, создать профиль или вернуть статус операции),
    
	- операция транзакционная и занимает миллисекунды/секунды,
    
	- взаимодействие происходит внутри одного bounded context.
    

	**Пример (ВкусВилл):**  
	При запросе на просмотр остатков пользовательский сервис через REST дергает **Inventory-Sync Service**, и сразу получает актуальные данные.

	**Пример (Серконс):**  
	Инженер запускает расчёт через **Formula-Engine** в Django → REST сразу возвращает ID задачи и статус.

	 🔹 Kafka

	Выбираю, когда:

	- реакция может быть отложена,
    
	- событие должно быть доступно сразу нескольким подписчикам,
    
	- важна история и возможность реплея.
    

	**Пример (ВкусВилл):**  
	При изменении остатков в ERP событие уходит в **Kafka** → его обрабатывают и **Inventory-Sync Service** (для витрины остатков), и **Anti-Fraud Service** (для анализа аномалий).

	**Пример (Серконс):**  
	ML-модуль в Django публиковал в Kafka событие о вероятности “провала” испытания → и Report-Service, и система мониторинга получали его независимо.

	RabbitMQ + Celery

	Применяю, когда:

	- операция тяжёлая или долгоживущая,
    
	- нужно гарантированное выполнение с ретраями и fallback,
    
	- задача не критична для UX (можно завершить позже).
    

	**Пример (ВкусВилл):**  
	Уведомления и фоновая аналитика шли через Celery → задача попадала в очередь RabbitMQ, и воркер забирал её даже спустя минуты.

	**Пример (Серконс):**  
	Генерация и шифрование протоколов испытаний выполнялись через Celery + RabbitMQ, так как это ресурсоёмкая операция, и пользователю не нужно было ждать её в онлайне.

	 Такой подход позволял балансировать между:

	- **быстрым UX** (REST),
    
	- **масштабируемым событийным обменом** (Kafka),
    
	- **надёжными тяжёлыми задачами** (RabbitMQ + Celery).

- С какими проблемами с брокерами конкретно столкнулись?

	- Во ВкусВилле я работал с Kafka и RabbitMQ, и на каждом были свои нюансы.  
	С Kafka мы столкнулись с тем, что в одном топике оказались и тяжёлые события из ERP, и лёгкие транзакции. В пике лёгкие события начинали задерживаться. Мы разделили топики и масштабировали консьюмеры отдельно — задержки ушли.

	С RabbitMQ были ситуации, когда очередь фоновых задач разрасталась, и часть сообщений застревала. Мы ввели TTL и dead-letter-очереди, чтобы система не блокировалась, и отдельные воркеры для «тяжёлых» задач.

	После этого поток событий стал стабильным: быстрые операции обрабатывались мгновенно, а тяжёлые задачи шли в своём ритме.

	 Подробный вариант (если попросят углубиться)

	**Kafka:**  
	Когда ERP и WMS публиковали изменения остатков, в одном топике шли и «тяжёлые» события синхронизации, и лёгкие обновления. При нагрузке это приводило к задержкам даже у лёгких событий. Решили это разделением топиков (`inventory.heavy` и `inventory.light`) и увеличением числа партиций. В итоге быстрые события доходили за миллисекунды, а тяжёлые обрабатывались параллельно, не мешая остальным.

	**RabbitMQ:**  
	С фоновой аналитикой и уведомлениями очередь иногда росла быстрее, чем её обрабатывали воркеры. Сообщения могли копиться и блокировать процесс. Мы настроили TTL и dead-letter-очереди, а также разделили воркеры по типам задач (уведомления отдельно, аналитика отдельно). Это позволило избежать блокировок и сделать процесс более предсказуемым.

- Доводилось ли проектировать архитектуру

	 Краткий ответ

	Да, мне не раз приходилось проектировать архитектуру — как отдельных сервисов, так и взаимодействия всей системы.  
	Во ВкусВилле я участвовал в проектировании микросервисов для синхронизации остатков, антифрода и мониторинга — продумывал схему обмена через REST и Kafka, а для фоновых задач использовал Celery + RabbitMQ.  
	В Серконсе проектировал DSL-модуль для расчётов и сервис генерации протоколов: определял модель данных, интеграции и схему очередей.


	### Развёрнутый ответ (если попросят детали)

	Во **ВкусВилле** я участвовал в проектировании архитектуры сразу нескольких сервисов.

	- Для **Inventory-Sync Service** я спроектировал схему работы: данные из ERP и WMS публиковались в Kafka, сервис агрегировал их и складывал в PostgreSQL/Redis, а другие системы получали результаты по REST.
    
	- В **Anti-Fraud Service** я продумал схему событий: транзакции уходили в Kafka, сервис обрабатывал их в связке с ML-моделью и публиковал обратно результаты.
    
	- Для **Monitoring Service** я построил архитектуру сбора метрик: сервисы отдавали данные по REST, а дальше они агрегировались в Prometheus и визуализировались в Grafana.  
	    Отдельно пришлось проектировать очередь фоновых задач на RabbitMQ с ретраями и fallback-механизмом.
    

	В **Серконсе** я спроектировал архитектуру DSL-модуля для инженеров: описывал модель данных в PostgreSQL, разработал API на Django и схему валидации показаний с датчиков DAQ.  
	Также участвовал в проектировании сервиса генерации протоколов: данные собирались в Report-Service, задачи уходили в RabbitMQ, воркеры занимались шифрованием и интеграцией с Госключ. Чтобы повысить отказоустойчивость, мы добавили dead-letter-очереди и отдельные воркеры для тяжёлых задач.

	Таким образом, я проектировал архитектуру как отдельных сервисов (с моделью данных, API и очередями), так и всей системы целиком — с выбором способа интеграции (REST, Kafka, RabbitMQ) и мониторингом.

- Как взаимодействую с кубером и как с докером

	- В наших проектах Kubernetes настраивали DevOps-инженеры, а я работал на уровне разработчика.  
	
	Мои задачи:

	- деплоить новые версии сервисов через готовые Helm-чарты,
    
	- смотреть логи и состояние подов через **kubectl** и OpenLens,
    
	- корректировать ресурсы в values-файлах и перекатывать сервис при узких местах,
    
	- временно масштабировать поды вручную или договариваться с DevOps про настройку HPA.
    

	**Живой пример (ВкусВилл):**  
	У нас была очередь в RabbitMQ, и когда нагрузка резко возрастала, задачи начинали задерживаться. Я через Grafana смотрел метрики, видел рост backlog и масштабировал воркеры через `kubectl scale`. Позже вместе с DevOps мы добавили автоскейлинг по нагрузке на очередь — это сделало систему стабильнее и предсказуемее.

	**Разграничение:** кластер, ingress, пайплайны и мониторинг были в зоне DevOps. Моя зона — **цикл “код → релиз → прод”** и поддержка стабильной работы сервисов на уровне разработчика.

	- Как я взаимодействую с Docker

		С Docker я работаю постоянно.

		- пишу **Dockerfile** (multi-stage, чтобы образы были компактными),
    
		- поднимаю окружение через **docker-compose** (приложение + PostgreSQL + Redis + Kafka + RabbitMQ),
    
		- использую `docker build`, `docker run`, `docker-compose up/down` для локальной разработки,
    
		- отлаживаю через `docker ps`, `docker logs`, `docker exec`.
    

		В CI/CD участвовал в настройке пайплайнов: сборка образа → пуш в registry → автотесты → деплой в Kubernetes.

	 Короткий устный вариант

	С Kubernetes я работал как разработчик: деплоил сервисы через Helm, смотрел логи, менял ресурсы и при нагрузке масштабировал поды. Во ВкусВилле был кейс: очередь в RabbitMQ переполнялась, и задачи застревали. Я масштабировал воркеры вручную через kubectl, а позже мы с DevOps настроили автоскейлинг по нагрузке на очередь.  
	С Docker работаю каждый день: пишу Dockerfile, поднимаю окружение через docker-compose, отлаживаю контейнеры и интегрирую образы в CI/CD.

- Как деплоили проекты
	- Мы собирали Docker-образы в GitLab CI и пушили их в registry. В пайплайне обычно было несколько стадий: линтеры (ruff, mypy) → тесты (pytest) → сборка образа и пуш → автоматический деплой в dev/stage → smoke-тесты.
    
	- На dev и stage деплоил я сам через Helm-чарты: проверял diff, поднимал сервис и смотрел /healthz, /ready. Если что-то падало, чинил values или откатывал через helm rollback.
    
	- В прод выкатывали DevOps-инженеры — у нас был manual gate: только после ручного подтверждения релиз попадал в прод. Это давало предсказуемость и возможность быстро откатиться, если что-то пошло не так.


		- Короткий устный вариант
    
	- Мы собирали образы в GitLab CI и пушили их в registry. В dev и stage я деплоил сам через Helm, а в проде — DevOps. 

- Как пишешь логи в своих приложениях
	- Я обычно использую стандартный модуль logging и иногда structlog для структурированных логов. Формат у нас JSON → пишу в stdout, что бы Docker и Kubernetes автоматически подхватывали и отправляли их в систему логирования (у DevOps это был ELK-стек / Grafana Loki).

	В каждый лог стараюсь добавлять уровень (INFO, ERROR) и полезный контекст — например, request_id, user_id, название сервиса. Конфигурацию задаю через dictConfig, чтобы было централизованно и одинаково для всех сервисов.

	Короткий устный вариант

	Я пишу JSON-логи через logging/structlog в stdout, добавляю request_id и user_id. 

- Как хранились логи?

	Логи у нас хранились централизованно. Все сервисы писали их в stdout в формате JSON (через logging/structlog), добавляя trace_id, user_id, название сервиса и уровень. В Kubernetes их собирал Fluent Bit и отправлял в Grafana Loki. Искали по trace_id или тегам сервиса в Grafana. Обычные логи хранились неделю, ошибки и критичные — до месяца, с выгрузкой в S3.

	-  Развёрнутый ответ

	**Формат логов**  
	Использовал стандартный Python logging с JSON-форматтером. В каждом сообщении было:

	- `timestamp`,
    
	- `level` (INFO/ERROR),
    
	- `trace_id` (пробрасывался через запрос),
    
	- `user_id` (если доступен),
    
	- `service`,
    
	- `message`.
    

	Пример:

	`{   "timestamp": "2025-05-26T12:34:56Z",   "level": "error",   "service": "anti-fraud-service",   "trace_id": "abc123",   "user_id": "u-78421",   "message": "Anomaly detected, ML-service timeout, fallback used" }`

	**Сбор логов**  
	В Kubernetes контейнеры писали в stdout. Fluent Bit собирал их с подов и отправлял в Loki.

	**Хранилище и доступ**

	- В Loki логи хранились **7 дней** для оперативного доступа.
    
	- Ошибки и критичные — до **30 дней**, после чего выгружались в **S3**.
    
	- Искали в Grafana через trace_id, user_id или тег сервиса.
    

	**Использование**

	- Во **ВкусВилле** часто искали проблемы в цепочке Kafka → Inventory-Sync → Anti-Fraud: по trace_id можно было отследить путь события.
    
	- В **Серконсе** по user_id поднимали все логи испытания и сразу видели, где зависла генерация протокола или отвалилась интеграция с Госключ.
    

	**Метрики из логов**  
	Из логов строились дашборды в Grafana: количество ошибок по сервисам, задержки синхронизации, ошибки при генерации протоколов. Это помогало видеть картину в динамике и быстро реагировать.


	Устный вариант (2 предложения)

	Сервисы писали JSON-логи в stdout, их собирал Fluent Bit и отправлял в Loki, искали через Grafana. Во ВкусВилле мы так отслеживали цепочку событий по trace_id, а в Серконсе находили ошибки при генерации протоколов и сбои в интеграциях.

- Как ты реализовывал роевую модель
	Ролевую модель мы строили на основе **RBAC (Role-Based Access Control)**.  
	При логине пользователь получал **JWT-токен** с его `user_id` и ролью (например, user, moderator, admin). Каждый сервис при запросе извлекал роль из токена и сверял её с матрицей доступа в **PostgreSQL**.  
	Правила можно было менять без перезапуска кода, а для скорости решения кэшировались в **Redis**. Если проверка проходила — запрос выполнялся, иначе возвращался **403 Forbidden**.


	 Детали реализации

	**JWT-токены**

	`{   "sub": "user_123",   "role": "moderator",   "exp": "..." }`

	Токен подписывался секретом и передавался в `Authorization: Bearer ...`.

	**Проверка прав**

	- сервис извлекал `role` из токена,
    
	- сверял с таблицей `access_rules` в PostgreSQL,
    
	- результат кешировался в Redis.
    

	Пример:

	- moderator → может `POST /content/review` ✅
    
	- user → не может `DELETE /profiles/{id}` ❌
    

	**Хранение политик**  
	В PostgreSQL была таблица `access_rules`:

	- role,
    
	- method,
    
	- path_pattern,
    
	- allowed.
    

	Правила можно было обновлять через админку, и новые права подхватывались без перезапуска.  
	Были вложенные роли: admin ⊃ moderator ⊃ user.

	**Логирование**  
	Каждая проверка логировалась: `user_id, role, path, method, result`. Это сильно помогало при разборе инцидентов.

	**Применение по компаниям**

	- Во **ВкусВилле** RBAC применялся в сервисах профилей и антифрода: централизованная проверка упростила код.
    
	- В **Серконсе** правила доступа к протоколам и расчётам были вынесены в отдельный слой, чтобы бизнес-логика не дублировала проверки.
    
	 Итог

	Ролевую модель мы реализовали через **JWT + централизованную проверку прав в отдельном слое**. Это дало гибкость, прозрачность и масштабируемость: бизнес-сервисы не хранили у себя проверки, а управлялись из одного места.

- Взаимодействие с брокерами сообщений.
	
	Я работал с тремя брокерами — **Kafka, Redis и RabbitMQ**, каждый решал свою задачу.

	### Kafka

	Kafka у нас была главной **событийной шиной** между сервисами.

	- Сервисы публиковали события через transactional outbox (например, `inventory.updated`, `loyalty.transaction`).
    
	- Подписчики в разных сервисах обрабатывали эти события: одни считали остатки, другие запускали антифрод, третьи обновляли витрину.
    
	- Масштабирование делалось добавлением **партиций** и **реплик консьюмеров**, а HPA в Kubernetes поднимал поды при росте consumer lag.

	- Даже при недоступности Kafka события сохранялись в outbox, поэтому не терялись.
    

	**Живой пример (ВкусВилл):**  
	Когда нагрузка на обновление остатков резко росла, consumer lag увеличивался, и антифрод начинал обрабатывать события с задержкой. Я временно масштабировал воркеры через `kubectl scale`, а позже вместе с DevOps мы настроили автоскейлинг по lag-метрике. После этого сервисы стали обрабатывать события стабильно даже в пике.


	### Redis

	Redis использовался не как основная очередь, а как быстрый инструмент для:

	- **pub/sub** — рассылки обновлений по WebSocket (например, изменения на дашбордах),
    
	- **rate-limit** — счётчики запросов с TTL в API Gateway (HTTP 429 при превышении),
    
	- **кэш** — хранение справочников и быстрых данных,
    
	- в **dev-окружениях** Redis временно подменял Kafka, чтобы не тянуть всю инфраструктуру.
    

	**Живой пример (ВкусВилл):**  
	В Anti-Fraud Service Redis применялся для кэширования промежуточных расчётов. Это позволило снизить нагрузку на Postgres и ускорить ответы при повторных запросах.


	### RabbitMQ
	
	В **Серконс** RabbitMQ был брокером для Celery, через него шли фоновые задачи:

	- генерация многоязычных протоколов испытаний,
    
	- шифрование и отправка на подпись в Госключ/CFCA,
    
	- уведомления сотрудникам.
    

	Особенности:

	- включали **ack/retry**, чтобы задачи не терялись,
    
	- разделяли очереди: тяжёлые протоколы отдельно, быстрые уведомления отдельно,
    
	- настраивали **prefetch_count**, чтобы воркеры брали задачи равномерно,
    
	- использовали **TTL и dead-letter** для «застрявших» сообщений.
    

	**Живой пример (Серконс):**  
	При массовой генерации протоколов очередь RabbitMQ разрасталась и замедляла систему. Мы настроили TTL для фоновых задач и dead-letter-очереди. В результате система стала предсказуемой: важные задачи выполнялись вовремя, а фоновые не блокировали процесс.

	### Итог

	- **Kafka** → основная событийная шина, масштабирование партициями и консьюмерами, гарантированная доставка.
    
	- **Redis** → быстрый pub/sub, rate-limit, кэш и упрощённый брокер в dev.
    
	- **RabbitMQ** → очередь для фоновых задач с ack/retry, dead-letter и балансировкой нагрузки.

	Таким образом, у меня есть практический опыт работы со всеми тремя брокерами и решения реальных проблем с каждым. 

- Какие тесты использовал?

	 Мы использовали несколько уровней тестирования:

	- Юнит-тесты на бизнес-логику (pytest, фикстуры, моки).
    
	- Интеграционные тесты через docker-compose (Postgres, Redis, Kafka, S3).  
      
	- Иногда нагрузочные тесты на k6, чтобы проверить критические эндпоинты.  
      
	- Дополнительно были QA-инженеры, которые проводили ручной регресс и exploratory-тесты.  
      
	Подробный ответ

	1. Юнит-тесты — писали pytest-тесты для проверки DTO, валидации, работы отдельных функций. Покрытие держали на уровне 70 %.  
      
	2. Интеграционные — поднимали окружение в docker-compose: Postgres + Redis + Kafka. Проверяли полный сценарий: API-запрос → запись в БД → событие в брокере → реакция воркера.  
      
	3. Нагрузочные — периодически гоняли k6, чтобы проверить стабильность API под нагрузкой (смотрели latency и количество ошибок).  
      
	4. QA — в команде был QA-инженер, который делал ручной регресс по чек-листам и exploratory-тестирование.  

	Итог

	Мы комбинировали автоматические тесты (unit + интеграция) с ручной проверкой от QA. Это давало достаточную уверенность, что сервисы работают стабильно и баги не вылезают в прод.

- Какой РПС был на работе? Как добились такого высокого рпс, что применяли для этого?

	##Краткий ответ

	Средняя нагрузка была около **300–350 RPS**, пиковая доходила до **800–900 RPS**. Мы держали её за счёт асинхронного **FastAPI**, вынесения тяжёлых задач в **Celery + Kafka/RabbitMQ**, кэша в **Redis** и грамотной настройки **PostgreSQL** (индексы, pgbouncer). Плюс **Kubernetes** автоматически скейлил поды при росте нагрузки.

	### Подробный ответ

	**Асинхронность**  
	В ВкусВилле API работал на **FastAPI + uvicorn**. REST-сервис занимался только валидацией и записью в БД, а тяжёлые операции (антифрод, агрегации, уведомления) сразу уходили в Kafka или в Celery-воркеры через RabbitMQ.

	**PostgreSQL**

	- Индексы по ключевым колонкам.
    
	- Connection-pool через **pgbouncer**, что держало p95-запросы в районе **10–15 мс**.
    
	- В Серконсе для ускорения поиска по документам использовал расширение **pg_trgm**: GIN-индекс с `gin_trgm_ops` позволил искать по подстроке и с опечатками. Запрос `ILIKE '%пожар%'` вместо секунды стал выполняться за **20 мс**.
    

	**Redis**  
	Использовался как кэш для справочников и быстрых данных, а также для rate-limit в API Gateway. Это сильно разгрузило Postgres.

	**Файлы**  
	В Серконсе документы сразу отправляли в S3 по **presigned URL**, чтобы не гонять файлы через бэкенд.

	**Горизонтальный скейл**  
	В Kubernetes у сервисов было по 2 реплики «вхолостую», но HPA поднимал до 10–12 при росте CPU или **consumer lag** в Kafka/RabbitMQ.

	**Разделение потоков**

	- Во ВкусВилле отчётные запросы и аналитика шли в read-реплики PostgreSQL, не мешая основной записи.
    
	- В Серконсе генерация протоколов шла в отдельные очереди RabbitMQ, чтобы не тормозить быстрые операции.
    

	Живой пример

	Во ВкусВилле при пиках нагрузки до ~900 RPS система оставалась стабильной: быстрые операции шли через FastAPI, тяжёлые события — через Kafka, а Redis и индексы в Postgres обеспечивали быстрые ответы.  
	В Серконсе внедрение **pg_trgm** позволило инженерам искать документы по части названия или фамилии автора без лагов: обычный `ILIKE` тянулся секунду, а с индексом поиск стал почти мгновенным.


	### Итог

	За счёт **асинхронности, кэша, индексов и горизонтального скейла** мы спокойно выдерживали пиковую нагрузку без скачков задержки, а использование pg_trgm в Postgres дало быстрый «живой поиск» без дополнительной инфраструктуры.

- Расскажи про ACID где использовал в проекте? Про каждую букву где именно использовались.

	## 📌 ВкусВилл (синхронизация остатков и антифрод)

	Краткий ответ  
	Во ВкусВилле ACID был критичен при синхронизации товарных остатков и обработке транзакций лояльности. В одной транзакции мы обновляли остатки в PostgreSQL, фиксировали бизнес-метрику и писали событие `inventory.updated` или `loyalty.transaction` в outbox для Kafka. Если хоть один шаг падал — всё откатывалось.

	- **A (Atomicity, атомарность):** обновление остатка и событие в Kafka фиксировались вместе через transactional outbox. Если одно не проходило → ROLLBACK.
    
	- **C (Consistency, согласованность):** ограничения `CHECK(stock >= 0)` и `UNIQUE(product_id, warehouse_id)` не позволяли уйти в минус или задвоить остаток.
    
	- **I (Isolation, изоляция):** при одновременной продаже одного товара использовали `SELECT … FOR UPDATE`, чтобы два процесса не изменили остаток одновременно.
    
	- **D (Durability, долговечность):** данные фиксировались в WAL PostgreSQL и реплицировались, поэтому даже при сбое сервера остатки не терялись.

	## 📌 Серконс (протоколы испытаний и расчёты)

	Краткий ответ  
	В Серконсе ACID был необходим при генерации протоколов и фиксации результатов испытаний. В одной транзакции мы записывали результаты формулы, создавали запись протокола и публиковали событие `report.generated` для дальнейшей подписи через Госключ.

	- **A (Atomicity):** результаты расчётов, запись протокола и событие фиксировались в одной транзакции. Если падал расчёт или генерация документа → всё откатывалось.
    
	- **C (Consistency):** внешние ключи не позволяли создать протокол для несуществующего испытания; `CHECK` не давал вносить значения вне ГОСТ/ISO диапазона.
    
	- **I (Isolation):** параллельные генерации одного и того же протокола блокировались на уровне строки (FOR UPDATE), чтобы избежать дубликатов.
    
	- **D (Durability):** протоколы фиксировались в WAL, реплицировались, плюс критичные документы сохранялись в зашифрованном виде и выгружались в S3, что гарантировало восстановление после аварии.

	## 📌 Итог (универсальная фраза)

	ACID для меня — это не теория, а рабочая практика.  
	Во ВкусВилле он защищал от дублей и рассинхрона остатков, а в Серконсе — от потери результатов испытаний и юридически значимых протоколов.

- Сколько стендов было и кто катил


	У нас было несколько основных стендов: Dev, Stage (или QA) и Prod.

	- Dev — личные фичи, туда катили сами разработчики через CI или вручную.  
    
	- Stage (QA) — копия продакшена с тестовыми данными; туда деплоил CI, а тестировщики гоняли регрессию и e2e.  
      
	- Prod — боевой, сюда выкатывали после ручного approve: обычно тимлид или дежурный DevOps.  
      
	Такой процесс позволял быстро проверять свои изменения локально и на dev, а уже потом идти по цепочке QA → прод.

	Подробно

	Dev

	- Каждый разработчик мог поднять сервис в dev-namespace или в docker-compose.  
    
	- Деплой запускался либо вручную (helm upgrade/docker compose up), либо через CI после merge в feature-ветку.  
      
	- На dev стенде можно было ломать схему БД, экспериментировать с API, подключать моковые сервисы.  
    
	Stage (QA)

	- Это была «полукопия» продакшена: та же конфигурация, но с тестовыми данными.  
    
	- Сюда CI/CD катил после прохождения тестов (lint, pytest, smoke).  
	- QA-инженеры прогоняли регрессию, e2e-сценарии и exploratory.  
    
	- Если что-то падало — стенд откатывали через helm rollback или перезапуск пайплайна.  
    
	Prod

	- Боевой контур, доступ ограничен.  
    
	- Выкатывался только из main-ветки, обязательно с ручным approve.  
      
	- Право запуска имели либо тимлид, либо дежурный DevOps.  
      
	- Если после деплоя шли алерты или метрики выходили за SLA, делали rollback (кнопкой в CI/CD).  

	Почему так

	- Dev — свобода для разработчиков.  
      
	- Stage/QA — зона ответственности тестировщиков, гарантия качества.  
      
	Prod — зона тимлида/DevOps, чтобы не пустить сырой код в боевую систему.

- Использовали S3, почему именно его, как использовал?

	Мы использовали **S3-совместимое хранилище** (в проде — Yandex Object Storage, в dev — MinIO). Оно идеально подходит для фото, документов и отчётов: масштабируется без ограничений, поддерживает версионирование и доступ по временным ссылкам (**presigned URL**). Это удобнее и надёжнее, чем хранить файлы в базе или на локальном диске.
	
	### 📌 Подробно

	**Почему именно S3**

	- У команды уже был опыт работы с ним → простая интеграция через `boto3`.
    
	- Подходит для файловых сценариев: отчёты, протоколы, документы, изображения.
    
	- Даёт нужные фичи «из коробки»:
    
	    - **Версионирование** — можно откатить протокол или документ к старой версии.
        
	    - **Lifecycle** — старые версии переводим в холодное хранилище, экономим деньги.
        
	    - **Presigned URL** — фронтенд и сервисы грузят файлы напрямую в облако, не нагружая бэкенд.

	**Как использовали**

	- Во **ВкусВилле**: отчёты, бизнес-документы и выгрузки отправляли в S3 по presigned URL. В базе хранили только ключи и метаданные, что разгружало PostgreSQL.
    
	- В **Серконсе**: многоязычные протоколы испытаний сохраняли в S3 с версионированием, чтобы можно было восстановить предыдущий вариант. Тяжёлые PDF и архивы не забивали базу и оставались доступными для подписи/отправки.

	### 📌 Итог

	S3 дало баланс: простая интеграция, надёжность, гибкость и низкая стоимость. Благодаря presigned URL фронт и мобильные клиенты работали напрямую с хранилищем, а бэкенд оставался лёгким и отвечал только за метаданные и контроль доступа.

-  Какой процент покрытия тестами был? Как измеряли процент покрытия?

	У нас среднее покрытие автотестами держалось на уровне ~60 %, при этом критическая бизнес-логика покрыта ближе к 80 %. Мы измеряли процент с помощью pytest-cov — он считает, какие строки кода реально исполнялись во время тестов, и в GitLab CI формировался отчёт по покрытию.

	Подробно

	- Что тестировали: юнит-тесты для бизнес-логики, интеграционные тесты с docker-compose (PostgreSQL, Redis, Kafka/MinIO), smoke-тесты после деплоя.  
      
	- Как измеряли:  
      
	- В CI/CD пайплайне запускался pytest --cov=src --cov-report=xml.  
      
	- pytest-cov собирал информацию о том, какие файлы и строки были выполнены.  
      
	- GitLab показывал процент покрытия в merge request → мы сразу видели, если новая фича снижает общий процент.  
      
	- Как росло покрытие:  
      
	- Начинали с ~45 %.  
      
	- За полгода подняли до ~60 % за счёт добавления контрактных тестов и покрытия «серых зон» (где раньше проверяли руками).  
      
	- Для ключевых мест (например, авторизация, биллинг, матчи в Twinby, загрузка файлов в Aston) держали планку 80 %+.  
      
	Итог: метрика покрытия помогала следить за качеством, но мы делали упор не на «гнаться за 100 %», а на то, чтобы реально тестировать критические сценарии, от которых зависит пользователь.

- С какими проблемами столкнулся при обновлении версии зависимостей, пример?

	При обновлении зависимостей основная проблема — это скрытые несовместимости. Например, при переходе на SQLAlchemy 2.0 у нас часть кода перестала работать: раньше можно было делать session.execute("RAW SQL"), а в новой версии это стало запрещено. Тесты часть покрыли, но в проде всё равно вылезли ошибки в воркерах. Пришлось быстро фиксить и переписывать вызовы на text("...").

	Подробно

	- SQLAlchemy:  

		- Обновление с 1.4 → 2.0.  
    
		- Старый код session.execute("SELECT ...") перестал работать.  
    
		- Решение: использовать from sqlalchemy import text и заменить вызовы на session.execute(text("SELECT ...")).  
      
		- Чтобы не ловить такие баги снова, добавили тесты на «сырые» запросы.  
      
	- aiohttp:  
      
		- В одном из minor-обновлений изменилось поведение таймаутов.  
      
		- Старый конфиг timeout=30 перестал действовать как раньше — запросы висели бесконечно.  
    
		- Решение: перейти на новый API ClientTimeout(total=30).  
      
	- Чему научился:  
      
	- Никогда не обновляю пачкой все зависимости. Сначала — dev-окружение, затем stage.  
    
	- На CI добавили job с pip list --outdated и проверкой changelog.  
      
		Теперь перед мажорным апдейтом поднимаем отдельный тестовый стенд.

- Что из технического долга последнее время успел выполнить?

	 Краткий ответ

	Последнее, что я закрыл из технического долга — это разделение одного большого Celery-воркера на несколько. Раньше в одном процессе крутились и push-уведомления, и OCR документов, и ночные отчёты. В пике OCR мог занимать десятки секунд, и из-за этого пуши задерживались на минуты. Я вынес задачи в разные очереди и отдельные воркеры, завёл разные HPA и алерты. После этого пуши стабилизировались (до сотен миллисекунд), а тяжёлые OCR больше не мешали realtime-событиям.

	Подробно (если начнут копать)

	До изменений:

	- Все три типа задач крутились в одном воркере Celery.  
      
	- OCR занимал по 20–40 секунд CPU, из-за этого пуши откладывались на минуту+.  
      
	- Ночные отчёты иногда уходили позже 08:00, бизнес жаловался.  
    
	- Kafka-лаг подскакивал до тысяч сообщений, алерты сыпались каждую неделю.  
      
	Что сделал:

	- Разделил воркеры:  
      
	- notifier-worker (push, email),  
      
	- document-worker (OCR + отчёты).  
      
	- Вынес задачи в отдельные топики Kafka.  
      
	- Настроил разные Helm-чарты и автоскейлинг (HPA): лёгкие push-воркеры масштабируются быстрее, OCR имеет отдельные лимиты CPU/RAM.  
      
	- Добавил отдельные метрики и алерты для каждого типа задач.  
	      
	Результат:

	- Push-уведомления приходят за ~200 мс, даже при пиках.  
      
	- OCR спокойно крутится в своём контуре и не мешает остальному.  
      
	- Отчёты стабильно формируются к утру.  
      
		Kafka-лаг перестал «взрываться».

- Как происходила рассылка уведомлений

	### Краткий ответ

	Уведомления рассылались асинхронно: событие фиксировалось в базе и публиковалось в очередь (**Kafka или RabbitMQ**) → отдельный сервис или воркер подхватывал задачу и отправлял уведомление (push, e-mail). **Redis** использовался для кэша токенов и счётчиков. Такой подход позволял API сразу отвечать пользователю, а сама рассылка выполнялась в фоне с гарантией доставки.

	### 📌 Подробно

	**ВкусВилл** (микросервисы на FastAPI)

	- Когда происходило событие (например, обновление профиля, начисление бонусов), сервис писал запись в PostgreSQL и публиковал событие в **Kafka**.
    
	- Сервис уведомлений подписывался на топики (`profile.updated`, `loyalty.bonus_added`), превращал событие в задачу и рассылал push или e-mail.
    
	- **Redis** использовался для хранения токенов устройств и ограничения частоты (rate-limit), чтобы один пользователь не перегрузил систему спамом.  
	    👉 Для пользователя это выглядело как мгновенный отклик: например, при начислении бонусов API сразу возвращал результат, а уведомление приходило пушем через несколько секунд.
    
	
	**Серконс** (сервис отчётов и протоколов)

	- Когда создавался протокол или завершалось испытание, событие фиксировалось в БД и публиковалось в **RabbitMQ**.
    
	- Celery-воркер забирал задачу и рассылал уведомления: письма инженерам о готовности протокола или статусы в CRM.
    
	- Redis кэшировал статусы и использовался для rate-limit.  
	    👉 Для пользователя это выглядело как письмо или уведомление в личном кабинете о том, что протокол готов и отправлен на подпись.

	### 📌 Итог

	- В **ВкусВилле** рассылка строилась на **Kafka + Redis**, что давало масштабируемость и мгновенный отклик.
    
	- В **Серконсе** — на **RabbitMQ + Celery + Redis**, что удобно для тяжёлых задач и гарантированной доставки писем.


	-  ## Как узнавали о неполадках лагах ошибках в проекте?


- Как узнавали о неполадках лагах ошибках в проекте?

	Мы узнавали о проблемах через мониторинг и алерты, а не от пользователей. Метрики собирал **Prometheus**: рост 5xx, задержки p95, лаги консьюмеров Kafka, глубина очередей RabbitMQ. **Alertmanager** слал уведомления в Slack/Telegram дежурному. Логи централизованно собирались в **Loki**, а по trace-id можно было восстановить цепочку запроса. Redis и Celery тоже мониторились: если очередь превышала лимит — прилетал алерт. Благодаря этому за пару минут было понятно, где «залипает» система, и можно было быстро реагировать.

	### 📌 Подробно
	
	**ВкусВилл**

	- **Метрики**: ошибки 5xx, задержка API, лаг в Kafka-консьюмерах, рост времени отклика PostgreSQL.
    
	- **Prometheus + Alertmanager**: если lag > 1000 сообщений или доля ошибок > 1 %, сразу прилетал алерт в Slack.
    
	- **Логи**: все сервисы писали JSON в stdout → Fluent Bit собирал → Loki. По trace-id можно было отследить путь события от REST-запроса до фоновой задачи.
    
	- **Реакция**: например, при росте lag в Kafka я открывал Grafana, видел, что воркеры не успевают, временно масштабировал поды через `kubectl scale`, после чего очередь разгружалась.

	**Серконс**

	- **RabbitMQ**: мониторили глубину очередей и скорость обработки задач. Если очередь росла слишком быстро, прилетал алерт.
    
	- **Celery**: следили за временем выполнения задач и количеством ретраев.
    
	- **PostgreSQL**: отслеживались долгие транзакции и блокировки.
    
	- **Бизнес-метрики**: например, резкий спад числа сгенерированных протоколов за час тоже считался сигналом.
    
	- **Реакция**: при алерте «очередь > 10k задач» проверяли Loki, находили причину (например, узкий индекс в БД), временно масштабировали воркеры, потом фиксили код/схему.

	### 📌 Итог

	Система мониторинга была построена так, что мы узнавали о проблемах заранее: **по метрикам, логам и алертам**, а не по жалобам пользователей.

	Хочешь, я соберу тебе ещё **короткий устный ответ (3–4 предложения)** для собеса, чтобы можно было сразу сказать без лишних деталей

- Что такое линтер

	Линтер — это инструмент, который анализирует исходный код программы с целью выявления потенциальных ошибок, проблем с форматированием, нарушений стиля кодирования или других недочетов. Линтер помогает поддерживать качество кода, предлагая рекомендации по улучшению, например, удаление неиспользуемых переменных или приведение кода к единому стилю. 

	Мы используем Ruff и Mypy.

- Все ли сервисы уходили в продакшн
	
	Да, все, но после деплоя в продакшн мы снимали с себя ответственность за разработку и поддержку, поэтому не знаю что было дальше с их судьбой.

- Сколько реплик было у проекта
	
	
	
	В базовой конфигурации у нас было **2–3 реплики на каждый сервис** — это давало устойчивость, если один под или нода падали. Для тяжёлых воркеров мы подключали **HPA (автоскейлер)**: если CPU уходил выше 70 % или накапливался лаг задач в Kafka/RabbitMQ, Kubernetes автоматически поднимал количество подов до **5–7**. Этого хватало, чтобы спокойно переживать пиковые нагрузки.


	### Развёрнутый ответ

	- **Стартовые реплики:** на большинстве сервисов было по **3 пода** (deployment replicas). Даже если один под отваливался, сервис оставался доступным.
    
	- **Ресурсы:** requests/limits задавали в Helm-values — например, `200m CPU / 256Mi RAM` на под.
    
	- **HPA:**
    
	    - `minReplicas=3`, `maxReplicas=7`,
        
	    - метрики: CPU ≥ 70 % или `kafka_consumer_lag > 1000` (во ВкусВилле),
        
	    - для RabbitMQ в Серконсе ориентировались на глубину Celery-очередей.
        
	- **Мониторинг:** в Grafana отслеживали лаги и загрузку CPU, а Alertmanager слал алерт в Slack/Telegram, если нагрузка держалась выше порога.
    
	- **Живой пример:**
    
	    - Во ВкусВилле при массовой рассылке пушей lag в Kafka резко рос. HPA автоматически поднимал число воркеров до 6–7, и очередь разгружалась за несколько минут.
        
	    - В Серконсе при генерации сотен протоколов одновременно RabbitMQ-очередь росла — мы видели алерт и вручную масштабировали воркеры, пока потом не настроили автоскейлинг.


- А как понимал/определял что надо увеличить реплики?

	Я масштабировал не «на глаз», а по метрикам. Смотрел на **CPU/RAM, p95 latency, consumer lag Kafka, глубину RabbitMQ-очередей**. Если показатели превышали SLA (например, CPU > 70–80 % или lag не падал), HPA автоматически поднимал поды до верхней границы, а при необходимости я временно увеличивал реплики вручную и фиксировал правило.
	
	### 📌 Что было сигналом «пора скейлить»

	- **CPU** ≥ 70–80 % стабильно 5+ минут на под.
    
	- **p95 latency** выше целевого (например, >200 мс для API) и растёт вместе с нагрузкой.
    
	- **Kafka**: consumer lag > 1000 и не снижается в течение 2–3 минут.
    
	- **RabbitMQ (Celery)**: глубина очередей растёт, ожидание задач > 1–2 секунд для «быстрых» очередей.
    
	- **Ошибки 5xx** > 1 % в течение нескольких минут.
    
	- Побочные признаки: OOM, частые рестарты подов, рост подключений к PostgreSQL через pgbouncer.

	### 📌 Что делал по шагам

	1. Проверял дашборды (Grafana): CPU, latency, lag, queue depth.
    
	2. Если это пиковая нагрузка, а код/индексы в норме → даю HPA добежать до maxReplicas или скейлю вручную.
    
	3. Если даже на верхней границе метрики «красные» → временно повышаю maxReplicas и ставлю задачу на оптимизацию (индекс в БД, кэш, батч-обработка).
    
	4. После стабилизации — возвращал лимиты к нормальным и фиксировал правило (ADR/вики: «при таких метриках — столько реплик»).

	### 📌 Мини-пример

	- **ВкусВилл**: при росте нагрузки на антифрод Kafka lag перевалил за 1000, воркеры не успевали. Я поднял реплики до 6–7, очередь разгрузилась, потом настроили автоскейлинг по lag-метрике.
    
	- **Серконс**: при массовой генерации протоколов RabbitMQ-очередь росла, задачи висели по 2–3 секунды. Я масштабировал воркеры вручную, потом добавили алерт на `task wait time`, чтобы реагировать заранее.


- Сколько весила одна партиция БД у вас и сколько их было?

	 Мы делили большие таблицы на партиции по дате или бизнес-ключу. В среднем одна партиция весила **30–50 ГБ**, а всего их было **десятки** (по месяцам/годам). Такой размер был оптимален: легко помещался в память для запросов и быстро обслуживался индексами.

	### 📌 Подробный ответ

	**ВкусВилл**

	- Самая тяжёлая таблица была по **транзакциям лояльности и продажам**.
    
	- Делили по дате (`created_at`), чтобы запросы типа «за последний месяц» шли только по одной партиции.
    
	- Одна партиция получалась ~**40–50 ГБ**, всего в бою держали **24–30 партиций** (2–2,5 года истории).
    
	- Старые партиции переводились в read-only и могли выгружаться в S3 для архива.
    

	**Серконс**

	- Таблицы с **протоколами испытаний и документами** тоже шли в партиционирование по дате загрузки.
    
	- Размер одной партиции был меньше — в районе **15–20 ГБ**, всего порядка **10–12 активных партиций**.
    
	- Такой подход ускорял поиск: новые данные всегда в «горячей» партиции, старые не мешали запросам.

	### 📌 Итог

	Мы старались держать партицию в пределах **20–50 ГБ** — это баланс между скоростью запросов и удобством администрирования. Всего было от **10 до 30 партиций**, в зависимости от таблицы и периода хранения.

## Метрики, Логи и производительность

- Как отслеживали метрики

	Краткий вариант

	Мы собирали метрики через Prometheus: снимали RPS, задержки (средние и p95/p99), ошибки по кодам 4xx/5xx, загрузку CPU и памяти. В Grafana у нас были дашборды и алерты в Slack. Для ошибок использовали Sentry, а для логов — Loki/ELK. Это давало и общую картину, и детали по конкретным сбоям.

	Развёрнутый вариант

	В проектах мы активно работали с метриками. Базовый набор снимал Prometheus: RPS, средняя задержка ответа, а также p95 и p99 (произношу как «девяносто пятый и девяносто девятый процентиль»), чтобы видеть «хвосты» самых медленных запросов. Мы следили за количеством ошибок по кодам 4xx и 5xx, загрузкой CPU и памяти. Все эти данные скрейпились Prometheus, а в Grafana у нас были дашборды по каждому сервису. На них настраивались алерты: если, например, p99 резко вырос или доля 5xx превысила порог, приходило уведомление в Slack или на e-mail.

	Для диагностики конкретных ошибок использовали Sentry — там удобно видеть stack trace, пользователя и контекст запроса. А для анализа логов и трейсов применяли Loki и ELK/OpenSearch-стек. Это позволяло с одной стороны видеть высокоуровневое состояние системы, а с другой — разбираться в конкретных инцидентах.

- Какие собирали/конспектировали метрики

	Мы собирали два слоя метрик: **бизнесовые и технические**.

	- Бизнесовые: количество заказов и транзакций лояльности (ВкусВилл), число сгенерированных протоколов и успешных подписей документов (Серконс).
    
	- Технические: p95/p99 задержки, ошибки 5xx, consumer lag в Kafka, глубина очередей RabbitMQ, загрузка CPU/памяти, репликационный лаг в PostgreSQL, ретраи Celery-тасков.  
	    Всё стекалось в **Prometheus**, дашборды были в **Grafana**, а алерты летели в Slack/Telegram. Для разбора деталей подключали **Sentry** и логи через **Loki**.


	### 📌 Развёрнутый вариант

	**Бизнесовые метрики**

	- Во **ВкусВилле**:
    
	    - количество новых заказов и успешных транзакций лояльности,
        
	    - доля успешно обработанных антифрод-событий,
        
	    - процент доставленных уведомлений.
        
	- В **Серконсе**:
    
	    - количество сгенерированных протоколов,
        
	    - процент успешных подписей через Госключ/CFCA,
        
	    - скорость формирования отчётов для инженеров.
        

	**Технические метрики**

	- **API**: RPS, p95/p99 latency, количество ошибок 4xx/5xx.
    
	- **PostgreSQL**: количество соединений, репликационный лаг, долгие транзакции.
    
	- **Kafka**: consumer lag, количество недореплицированных партиций.
    
	- **RabbitMQ/Celery**: глубина очередей, время ожидания задач, количество ретраев.
    
	- **Kubernetes**: CPU, память, количество рестартов подов.
    

	**Как использовали**

	- Все данные собирал **Prometheus**, дашборды строили в **Grafana**.
    
	- **Alertmanager** слал уведомления в Slack/Telegram, например:
    
	    - «5xx > 1 % три минуты подряд»
        
	    - «Kafka lag > 1000 сообщений»
        
	    - «очередь RabbitMQ > 10k задач».
        
		- Для детализации падений подключали **Sentry** (stack trace, контекст запроса) и **логи через Loki** (по trace_id восстанавливали путь запроса)
	### 📌 Итог

	Мы всегда держали баланс: бизнесовые метрики показывали влияние на пользователей, а технические — где именно узкое место. Такой подход позволял заранее ловить проблемы и быстро их локализовать.

- Как смотрел логи сервисов? Как хранились логи?

	Все сервисы писали JSON-логи в **stdout** с уровнем (INFO/ERROR), `trace_id`, `user_id`, названием сервиса и сообщением. **Kubernetes** через Fluent Bit собирал их и отправлял в центральное хранилище: во ВкусВилле — **Loki + Grafana**, в Серконсе — **ELK (Elasticsearch + Logstash + Kibana)**. Для быстрой отладки смотрел свежие логи через `kubectl logs`, а ошибки дополнительно отслеживались в **Sentry**.

	### 📌 Подробно

	**Формат логов**

	- Использовали стандартный `logging` или `structlog` в JSON-формате.
    
	- В каждом сообщении было:
    
	    - `timestamp`,
        
	    - `level`,
        
	    - `service`,
        
	    - `trace_id`,
        
	    - `user_id` (если доступен),
        
	    - `message`.
        

	**Сбор логов**

	- Во **ВкусВилле**: логи из pod-ов собирал **Fluent Bit** → отправлял в **Loki** → искали через **Grafana**.
    
	- В **Серконсе**: настроен классический **ELK-стек** (Elasticsearch + Logstash + Kibana).
    

	**Как пользовался**

	- Для инцидентов: открывал Grafana/Kibana и искал по `trace_id` или `user_id` всю цепочку запроса.
    
	- Для быстрой проверки: заходил в Kubernetes и смотрел логи прямо командой `kubectl logs <pod>`.
    
	- Для ошибок: подключён **Sentry** — там помимо stack trace видно было пользователя, контекст запроса и дополнительные параметры.
    
	**Хранение логов**

	- «Обычные» логи держались **7 дней** в Loki/ELK.
    
	- Ошибки и critical-level — дольше, до **30 дней**, потом архивировались в **S3**.

	### 📌 Итог

	Я работал с логами на двух уровнях:

	- **оперативно** (через `kubectl logs` и Sentry),
    
	- **исторически** (через Loki/Grafana или Kibana).
    

	Такой подход давал и быстрый доступ для отладки, и долгосрочное хранение для расследований.

- Какие виды документации были в проекте?

	В проектах мы держали документацию на нескольких уровнях.

	1. Confluence — это было основное хранилище архитектуры и процессов. Там были C4-диаграммы, ADR-файлы («почему выбрали Kafka», «почему вынесли профили в отдельный сервис»), инструкции по деплою, пост-мортемы и онбординг для новичков. Новичок реально мог за пару дней поднять сервисы, просто следуя гайду.  
      
	2. README.md — в каждом репозитории был свой README с инструкцией: установка зависимостей, команды make dev / make test, docker-compose для локального запуска PostgreSQL/Kafka/Redis. Это был первый вход для стажёра или джуна.  
      
	3. Docstring-и в коде — мы писали docstring-и в стиле Google. Это упрощало ревью: сразу видно, что делает функция, какие аргументы принимает, какие исключения кидает.  
      
	4. Swagger (OpenAPI) — FastAPI автоматически публиковал спеки на /docs и /openapi.json. QA проверяли их через Schemathesis, фронтендеры брали JSON и генерировали TypeScript-клиент. Если схема менялась, CI падал, пока не обновят спецификацию.  
      
    

	В итоге: Confluence покрывал процессы и архитектуру, README — локальный запуск, docstring-и помогали в коде, Swagger гарантировал синхронность фронта и бэка.

- Какой был гитфлоу?

	В проектах мы придерживались упрощённого Git-Flow без отдельной develop.

	- Ветки:  

	- main — продакшен-ветка, всегда держали её зелёной и готовой к релизу.  
      
	- feature/... — рабочие ветки для задач, обычно называли по тикету из Jira или GitLab Issues, например feature/DEF-142-analytics.  
      
	- hotfix/... — для срочных багфиксов прямо из main.  
      
	- Merge Request:  
	    Каждый MR проходил пайплайн: линтеры (ruff, mypy), тесты (pytest), сборка Docker-образа. Без зелёного пайплайна и минимум одного аппрува от коллеги merge был невозможен.  
      
	- Деплой:  
	      
	- Dev — можно было поднять сразу из feature-ветки (deploy-dev).  
      
	- Staging — автоматически после мержа в main, плюс запускались smoke- и контракт-тесты.  
    
	- Prod — только manual-approve, выкладка через Helm/Argo, часто канарейкой (например, 5% → 50% → 100%).  
      
	- Релизы:  
	    Каждый прод-релиз помечался тегом vX.Y.Z. При необходимости отката использовали либо helm rollback, либо git revert и новый тег.  
	- Hotfix:  
	    Если баг критичный, создавали hotfix/..., фикс сразу шёл в main, CI прогонял тесты и код деплоился напрямую в прод.  
      
	Почему так: отказались от develop и лишних веток, чтобы быстрее катить фичи. Такой поток давал баланс: код всегда стабилен в main, а выкладки можно делать несколько раз в неделю без хаоса.

- ЭНДПОИНТ на бэке работает особо медленно

	Если эндпоинт замедлился — сначала **измеряю** (APM / trace), потом убираю лёгкие узкие места в коде/БД (EXPLAIN, индексы, убрать N+1), кеширую, и только затем **масштабирую**. Так: `замер → оптимизация → кэш → масштаб`.

	# Пошаговый алгоритм (под твой опыт — ВкусВилл / Серконс)

	1. **Измерить (APM / трассировка)**  
	    Открываю трейс в **Jaeger / Elastic APM** — смотрю, где теряется время: приложение, БД или внешние вызовы. Для SQL запускаю `EXPLAIN ANALYZE` (ищу Seq Scan / большие join’ы / N+1). В ВкусВилле так нашли N+1 при выборке профилей и убрали его.
    
	2. **Проверка кода и асинхронности**  
	    Ищу блокировки в async-эндпоинтах (вызов `requests` в async, CPU-bound операции). Заменяю на `httpx.AsyncClient()` или выношу в **Celery**. Проверяю пул соединений к БД (connection pool / pgbouncer).
    
	3. **Оптимизация БД**  
	    Индексы по WHERE/JOIN, убрать `SELECT *`, заменить `OFFSET` на keyset-pagination, переписать медленные подзапросы в JOIN. В Серконсе отчёт упал с ~60 с до 2 с после индекса и рефакторинга запроса.
    
	4. **Кэширование**  
	    Кеш в **Redis** или `functools.lru_cache` для часто повторяющихся данных (справочники, метаданные). Во ВкусВилле кэшировал справочники — выдача стала миллисекундной.
    
	5. **Внешние вызовы**  
	    Если узкое место — внешний сервис, выношу работу в асинхронную задачу: возвращаю `202 Accepted`, а результат отдаю через WebSocket / push.
    
	6. **Масштабирование**  
	    Если оптимизации не хватает — масштабируем: поднимаю количество реплик (HPA / `kubectl scale`). Для CPU-bound настраиваю число gunicorn workers = ядра, для I/O-bound — 2–3 на ядро. Celery/Celery-воркеры скейлятся по lag/queue depth.
    
	7. **Крайние меры / инциденты**  
	    Проверяю pg_locks / deadlocks, уровень изоляции; если брокер забит — добавляю реплики и настраиваю back-pressure; фиксирую долгосрочные решения (индексы, батчинг, ADR).
    

	# Что именно смотрю как сигнал «пора скейлить»

	- CPU ≥ 70–80% стабильно 5+ минут (и нет долгих DB-запросов).
    
	- p95 latency выше целевой (например >200 ms) и растёт вместе с нагрузкой.
    
	- Kafka consumer lag >1000 и не падает (2–3 мин).
    
	- RabbitMQ / Celery: queue depth растёт, task wait >1–2 s на быстрых очередях.
    
	- 5xx ошибки >1% несколько минут. Побочные: OOM, частые рестарты, pgbouncer на потолке.
    

	# Быстрый чек-лист команд / проверок

	- APM: открыть трейс в **Jaeger / Elastic APM**.
    
	- SQL: `EXPLAIN ANALYZE <query>` → искать Seq Scan / expensive joins.
    
	- Логи: `kubectl logs -f <pod>` / поиск по `trace_id` в Loki.
    
	- Kafka: посмотреть consumer lag (метрика).
    
	- БД блокировки: `SELECT * FROM pg_locks;` / check long transactions.
    
	- Временное скейление: `kubectl scale deployment/<name> --replicas=6`.
    

	# Elevator pitch для собеса (30–40 сек)

	Если эндпоинт начал тормозить, я сначала **замеряю** в APM (Jaeger/Elastic) и локализую узкое место — код, БД или внешний сервис. Затем устраняю лёгкие узкие места (убираю N+1, добавляю индекс, меняю синхронные вызовы на async или выношу в Celery), кеширую горячие данные и только после этого масштабирую поды по метрикам (CPU/lag/queue depth). Такой порядок «замер → оптимизация → кэш → масштаб» даёт предсказуемый и экономный результат.

- ФРОНТ работает быстрее, чем БЭК
	
	Да, у нас такое бывало и во ВкусВилле, и в Серконсе. Чтобы фронт не простаивал, мы заранее договаривались о **API-контрактах** и описывали их в **Swagger (OpenAPI)**.

	### 📌 Как решали

	1. **API-контракт**
    
	    - Вместе с фронтендерами описывали эндпоинты, параметры и форматы ответов в Swagger.
        
	    - Это становилось «истиной» — и для фронта, и для бэка.
        
	2. **Мок-сервер**
    
	    - Поднимали мок по спецификации (часто через генератор, например `prism` или встроенные тулзы Swagger).
        
	    - Фронтендеры подключали его и спокойно работали: верстали, тестировали сценарии, писали автотесты.
        
	3. **Параллельная работа**
    
	    - Пока фронт использовал мок, мы на бэке достраивали бизнес-логику: интеграции с PostgreSQL, Kafka/RabbitMQ, внешними API.
        
	    - В Серконсе это были протоколы и подписи через Госключ, во ВкусВилле — антифрод, лояльность и профили.
        
	4. **Интеграция**
    
	    - Когда сервис был готов, фронт просто менял адрес в настройках с мока на реальный URL.
        
	    - Дальше шли интеграционные тесты и проверка end-to-end.

	### 📌 Итог

	Такой подход позволял работать **параллельно**, а не ждать, пока «догонит» бэк. В результате фронт не простаивал, фичи выпускались быстрее, а баги ловились ещё на этапе моков.

- Самая сложная проблема, и как ее решал

	**Проблема (ВкусВилл)**  
	При массовых акциях резко возрастала нагрузка: Kafka-топик с событиями транзакций и уведомлений рос быстрее, чем воркеры успевали обрабатывать. В итоге **consumer lag** достигал тысяч сообщений, пользователи начинали получать уведомления с задержкой в минуты.

	**Что делал**

	1. Сначала посмотрел метрики в Grafana: lag, CPU воркеров, глубину очередей.
    
	2. Проверил, что сама бизнес-логика не тормозит — SQL-запросы оптимизированы, индексы на месте.
    
	3. Временно руками увеличил количество воркеров (`kubectl scale`), чтобы разгрузить очередь.
    
	4. Вместе с DevOps мы добавили **HPA по lag-метрике** Kafka. Теперь при росте lag поды автоматически поднимались, а после пика нагрузка спадала — реплики снижались.
    

	**Результат**

	- Уведомления стали приходить стабильно даже при пиковых нагрузках.
    
	- Lag не превышал допустимого порога, пользователи не замечали задержек.
    
	- Мы зафиксировали правило и вынесли опыт в документацию.

	№2

	### 📌 Проблема

	Мы столкнулись с тем, что при больших объёмах заказов начали «падать» отчётные запросы к PostgreSQL. Аналитика и витрины дергали тяжёлые JOIN’ы и агрегации прямо с продовой базы. В итоге **пики чтения блокировали запись заказов**, и пользователи в приложении начали получать ошибки при оплате

	### 📌 Как решал

	1. **Диагностика**: в APM и `pg_stat_activity` увидел, что тяжёлые SELECT с агрегациями держат блокировки, а p95 latency API выросла в 2–3 раза.
	    
	2. **Быстрый фикс**: временно ограничили самые прожорливые отчёты и переключили их на ночное выполнение.
    
	3. **Архитектурное решение**:
    
	    - вынесли аналитику в отдельный сервис (**Analytics Service**),
        
	    - подключили **read-реплики PostgreSQL** для тяжёлых SELECT,
        
	    - для событийного сбора данных начали использовать Kafka → ClickHouse.
        
	4. **Оптимизация SQL**: переписал пару особенно медленных запросов — добавил индексы и перевёл часть выборок на keyset-пагинацию.

	### 📌 Результат

	- Основная база перестала блокироваться, API стабилизировалось.
    
	- Отчётные сервисы получили свою read-реплику и ClickHouse, где тяжёлые выборки стали работать мгновенно.
    
	- Пользователи больше не видели ошибок при заказах даже в нагрузку.

	👉 Итог: самая сложная проблема здесь была в **конкуренции аналитики и продовых транзакций**. Решили её комбинацией архитектуры (реплики, отдельный сервис, Kafka+ClickHouse) и оптимизацией SQL.

	### Серконс
	
	**В Серконсе** похожая проблема была с RabbitMQ: при генерации сотен протоколов очередь забивалась, и задачи висели часами. Мы настроили **TTL и dead-letter**, разделили тяжёлые и лёгкие очереди, добавили автоскейл воркеров. После этого даже пиковые генерации не блокировали систему.

	👉 Итог: моя самая сложная проблема была связана с **брокерами и нагрузкой**, и я её решал поэтапно: **анализ → временный фикс → автоматизация → документация**.

	Короткие

	## Пример 1. Kafka — задержки уведомлений (ВкусВилл)

	**Проблема:** при пиках нагрузок consumer lag в Kafka резко рос, и пользователи начали получать уведомления с задержкой в минуты.  
	**Решение:** временно масштабировал воркеры вручную через `kubectl scale`, затем вместе с DevOps внедрил **автоскейлинг по lag-метрике**.  
	**Результат:** уведомления приходили стабильно, даже при акциях и массовых нагрузках.

	---

	## 🔹 Пример 2. RabbitMQ — очередь протоколов (Серконс)

	**Проблема:** при генерации сотен протоколов RabbitMQ-очередь разрасталась до десятков тысяч задач, и фоновые процессы зависали.  
	**Решение:** ввёл **TTL и dead-letter-очереди**, разделил тяжёлые и лёгкие задачи по разным очередям, настроил prefetch и автоскейл воркеров.  
	**Результат:** система стала предсказуемой, тяжёлые задачи не блокировали быстрые уведомления.

	---

	## 🔹 Пример 3. PostgreSQL — конкуренция аналитики и продовых транзакций (ВкусВилл)

	**Проблема:** отчётные запросы (тяжёлые JOIN/агрегации) выполнялись прямо на продовой базе и блокировали запись заказов. Пользователи начали получать ошибки при оплате.  
	**Решение:**

	- вынес аналитику в отдельный сервис (**Analytics Service**),
    
	- подключили **read-реплики PostgreSQL**,
    
	- для событийного сбора использовали **Kafka → ClickHouse**,
    
	- переписал SQL-запросы (индексы, keyset-пагинация).  
	    **Результат:** база стабилизировалась, API перестало лагать, аналитика выполнялась быстро и отдельно.
    

	---

	## 🔹 Пример 4. Внешние сервисы и таймауты (Серконс)

	**Проблема:** при интеграции с Госключ (подпись протоколов) внешний сервис периодически «подвисал» и давал таймауты, из-за чего блокировались основные воркеры.  
	**Решение:**

	- вынес вызовы в **асинхронные Celery-задачи**,
    
	- добавил **retry с экспоненциальной задержкой**,
    
	- сделал fallback: если сервис недоступен, задача ставилась в отложенную очередь.  
	    **Результат:** система продолжала работать стабильно, даже если Госключ был недоступен несколько минут.

- Самая интересная задача, которую решал